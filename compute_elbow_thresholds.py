#!/usr/bin/env python3
"""
Compute elbow thresholds for model activations.

Usage:
    python compute_elbow_thresholds.py MODEL_NAME \
        --hookpoints "layers.[0-10].self_attn.o_proj" \
        --num_tokens 10000000 \
        --output thresholds.json
"""
import argparse
import json
from collections import defaultdict
from pathlib import Path
from multiprocessing import Pool, cpu_count
from functools import partial

import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
import matplotlib.pyplot as plt
import matplotlib

from sparsify.data import MemmapDataset
from sparsify.trainer import expand_range_pattern
from sparsify.utils import get_layer_list
from fnmatch import fnmatchcase
from natsort import natsorted
import ml_dtypes

# ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼ˆé¿å…æ˜¾ç¤ºçª—å£ï¼‰
matplotlib.use('Agg')

def compute_kneedle_elbow(data: np.ndarray, max_percentile: float = 0.95) -> dict[str, float]:
    """
    è®¡ç®—Kneedleæ‹ç‚¹ï¼ˆåªä¿å­˜elbowä¿¡æ¯ï¼Œä¸è®¡ç®—thresholdï¼‰

    Args:
        data: æ¿€æ´»æ•°æ®æ•°ç»„
        max_percentile: æœ€å¤§åˆ†ä½æ•°ï¼ˆè¿‡æ»¤æç«¯å€¼ï¼‰

    Returns:
        åŒ…å«elbow_p, elbow_valueçš„å­—å…¸ï¼ˆä¸åŒ…å«thresholdï¼Œç”±ç”¨æˆ·æŒ‡å®šalphaè®¡ç®—ï¼‰
    """
    # è½¬æ¢ä¸ºç»å¯¹å€¼å¹¶å±•å¹³
    x_abs = np.abs(data).flatten()

    # è®¡ç®—è¿‡æ»¤åçš„åˆ†ä½æ•°æ›²çº¿ï¼ˆP0-max_percentileï¼‰
    num_points = 500
    percentiles = np.linspace(0, max_percentile, num_points)
    quantiles = np.quantile(x_abs, percentiles)

    # Kneedleç®—æ³•ï¼šæ‰¾åˆ†ä½æ•°æ›²çº¿åç¦»é¦–å°¾è¿çº¿æœ€è¿œçš„ç‚¹
    p_start, p_end = percentiles[0], percentiles[-1]
    q_start, q_end = quantiles[0], quantiles[-1]

    # è®¡ç®—æ¯ä¸ªç‚¹åˆ°ç›´çº¿çš„è·ç¦»
    distances = np.zeros_like(percentiles)
    for i, (p, q) in enumerate(zip(percentiles, quantiles)):
        numerator = (q_end - q_start) * (p - p_start) - (p_end - p_start) * (q - q_start)
        denominator = np.sqrt((p_end - p_start)**2 + (q_end - q_start)**2)
        distances[i] = abs(numerator / denominator)

    # æ‰¾æœ€å¤§è·ç¦»ï¼ˆæ’é™¤å‰å5%çš„è¾¹ç•Œï¼Œé¿å…æ•°å€¼è¯¯å·®ï¼‰
    margin = 0.05 * (percentiles[-1] - percentiles[0])
    mask = (percentiles > percentiles[0] + margin) & (percentiles < percentiles[-1] - margin)
    valid_indices = np.where(mask)[0]

    if len(valid_indices) == 0:
        raise RuntimeError(
            f"âŒ Kneedleé˜ˆå€¼æ£€æµ‹å¤±è´¥ï¼šæ²¡æœ‰æœ‰æ•ˆçš„æ‹ç‚¹å€™é€‰ï¼\n"
            f"   æ•°æ®ç»Ÿè®¡: min={x_abs.min():.4f}, max={x_abs.max():.4f}, "
            f"mean={x_abs.mean():.4f}, std={x_abs.std():.4f}"
        )

    max_idx = valid_indices[np.argmax(distances[valid_indices])]
    max_distance = distances[max_idx]

    # æ£€æŸ¥è·ç¦»æ˜¯å¦å¤ªå°ï¼ˆå¯èƒ½è¡¨ç¤ºæ²¡æœ‰æ˜æ˜¾æ‹ç‚¹ï¼‰
    if max_distance < 1e-6:
        raise RuntimeError(
            f"âŒ Kneedleé˜ˆå€¼æ£€æµ‹å¤±è´¥ï¼šæœªæ‰¾åˆ°æ˜æ˜¾æ‹ç‚¹ï¼\n"
            f"   æœ€å¤§åç¦»è·ç¦»: {max_distance:.2e}\n"
            f"   æ•°æ®ç»Ÿè®¡: min={x_abs.min():.4f}, max={x_abs.max():.4f}, "
            f"mean={x_abs.mean():.4f}, std={x_abs.std():.4f}"
        )

    elbow_p = percentiles[max_idx]
    elbow_value = quantiles[max_idx]

    return {
        'elbow_p': float(elbow_p),
        'elbow_value': float(elbow_value)
    }


def plot_elbow_curve(data: np.ndarray, result: dict, name: str, output_path: Path, max_percentile: float = 0.95):
    """
    ç»˜åˆ¶æ¿€æ´»å€¼åˆ†å¸ƒå’Œelbow point

    Args:
        data: æ¿€æ´»æ•°æ®æ•°ç»„
        result: compute_kneedle_elbowçš„è¿”å›ç»“æœ
        name: hookpointåç§°
        output_path: å›¾ç‰‡ä¿å­˜è·¯å¾„
        max_percentile: æœ€å¤§åˆ†ä½æ•°
    """
    # è®¡ç®—åˆ†ä½æ•°æ›²çº¿ï¼ˆå’Œcompute_kneedle_elbowç›¸åŒï¼‰
    x_abs = np.abs(data).flatten()
    num_points = 500
    percentiles = np.linspace(0, max_percentile, num_points)
    quantiles = np.quantile(x_abs, percentiles)

    # Kneedleç®—æ³•çš„é¦–å°¾è¿çº¿
    p_start, p_end = percentiles[0], percentiles[-1]
    q_start, q_end = quantiles[0], quantiles[-1]

    # åˆ›å»ºå›¾å½¢
    fig, ax = plt.subplots(figsize=(10, 6))

    # ç»˜åˆ¶åˆ†ä½æ•°æ›²çº¿
    ax.plot(percentiles, quantiles, 'b-', linewidth=2, label='Quantile Curve')

    # ç»˜åˆ¶é¦–å°¾è¿çº¿
    ax.plot([p_start, p_end], [q_start, q_end], 'r--', linewidth=1.5,
            alpha=0.7, label='Linear Baseline')

    # æ ‡æ³¨elbow point
    elbow_p = result['elbow_p']
    elbow_value = result['elbow_value']
    ax.plot(elbow_p, elbow_value, 'ro', markersize=12, label='Elbow Point',
            zorder=5)

    # æ·»åŠ elbow pointçš„å‚ç›´çº¿å’Œæ–‡å­—æ³¨é‡Š
    ax.axvline(x=elbow_p, color='gray', linestyle=':', alpha=0.5)
    ax.annotate(f'Elbow Point\np={elbow_p:.3f}\nvalue={elbow_value:.6f}',
                xy=(elbow_p, elbow_value),
                xytext=(elbow_p + 0.1, elbow_value + (q_end - q_start) * 0.1),
                fontsize=10,
                bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),
                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))

    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel('Percentile', fontsize=12)
    ax.set_ylabel('Activation Value (Absolute)', fontsize=12)
    ax.set_title(f'Activation Distribution: {name}', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10, loc='upper left')
    ax.grid(True, alpha=0.3)

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†
    stats_text = (
        f'Data Statistics:\n'
        f'Min: {x_abs.min():.6f}\n'
        f'Max: {x_abs.max():.6f}\n'
        f'Mean: {x_abs.mean():.6f}\n'
        f'Std: {x_abs.std():.6f}\n'
        f'Samples: {len(x_abs):,}'
    )
    ax.text(0.02, 0.98, stats_text,
            transform=ax.transAxes,
            fontsize=9,
            verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    # ä¿å­˜å›¾ç‰‡
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close(fig)


def compute_elbow_for_layer(item, max_percentile, plot_dir=None):
    """
    å¹¶è¡Œè®¡ç®—å•ä¸ªlayerçš„elbowï¼ˆç”¨äºmultiprocessingï¼‰

    Args:
        item: (name, acts) tuple
        max_percentile: æœ€å¤§åˆ†ä½æ•°
        plot_dir: å¦‚æœæä¾›ï¼Œåˆ™ä¿å­˜å¯è§†åŒ–å›¾ç‰‡ï¼ˆå­—ç¬¦ä¸²è·¯å¾„ï¼‰

    Returns:
        (name, result, error) tuple
    """
    name, acts = item
    try:
        result = compute_kneedle_elbow(acts, max_percentile)

        # å¦‚æœæŒ‡å®šäº†plot_dirï¼Œç»˜åˆ¶å›¾ç‰‡
        if plot_dir is not None:
            plot_dir_path = Path(plot_dir)  # è½¬æ¢ä¸ºPathå¯¹è±¡
            plot_path = plot_dir_path / f"{name.replace('.', '_')}.png"
            print(f"      ğŸ“Š Plotting {name} -> {plot_path}")  # è°ƒè¯•ä¿¡æ¯
            plot_elbow_curve(acts, result, name, plot_path, max_percentile)

        return (name, result, None)  # (name, result, error)
    except Exception as e:
        import traceback
        error_msg = f"{str(e)}\n{traceback.format_exc()}"
        return (name, None, error_msg)  # (name, result, error)


class ActivationCollector:
    """æ”¶é›†æ¨¡å‹æ¿€æ´»å€¼"""

    def __init__(self, model, hookpoints: list[str], max_tokens: int, special_token_ids: set = None):
        self.model = model  # å®Œæ•´çš„æ¨¡å‹ï¼ˆç”¨äºå‰å‘ä¼ æ’­ï¼‰
        self.base_model = model.base_model if hasattr(model, 'base_model') else model
        self.hookpoints = hookpoints
        self.max_tokens = max_tokens
        self.special_token_ids = special_token_ids or set()  # ç‰¹æ®Štoken IDs (BOS, EOS, etc.)
        self.activations = defaultdict(list)
        self.attention_masks = []  # ä¿å­˜attention masks
        self.input_ids_list = []  # ä¿å­˜input_idsç”¨äºè¿‡æ»¤ç‰¹æ®Štokens
        self.tokens_collected = 0
        self.handles = []

    def _make_hook(self, name: str):
        """åˆ›å»ºhookå‡½æ•°"""
        def hook(module, input, output):
            if self.tokens_collected >= self.max_tokens:
                return

            # å¤„ç†è¾“å‡ºï¼ˆå¯èƒ½æ˜¯tupleï¼‰
            if isinstance(output, tuple):
                output = output[0]

            # è½¬æ¢ä¸ºfloat32å†è½¬numpyï¼ˆnumpyä¸æ”¯æŒbfloat16ï¼‰
            if output.dtype == torch.bfloat16:
                output = output.float()

            # è½¬æ¢ä¸ºnumpyå¹¶å­˜å‚¨ï¼ˆä½¿ç”¨CPUèŠ‚çœGPUå†…å­˜ï¼‰
            act = output.detach().cpu().numpy()
            self.activations[name].append(act)

        return hook

    def register_hooks(self):
        """æ³¨å†Œæ‰€æœ‰hooks"""
        for name in self.hookpoints:
            module = self.base_model.get_submodule(name)  # ä½¿ç”¨base_model
            handle = module.register_forward_hook(self._make_hook(name))
            self.handles.append(handle)

    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰hooks"""
        for handle in self.handles:
            handle.remove()
        self.handles.clear()

    def collect(self, dataloader, device):
        """æ”¶é›†æ¿€æ´»å€¼"""
        self.model.eval()
        self.register_hooks()

        try:
            with torch.no_grad():
                pbar = tqdm(dataloader, desc="Collecting activations")
                for batch in pbar:
                    if self.tokens_collected >= self.max_tokens:
                        break

                    input_ids = batch["input_ids"].to(device)
                    attention_mask = batch.get("attention_mask", None)
                    if attention_mask is not None:
                        attention_mask = attention_mask.cpu().numpy()  # è½¬åˆ°CPU

                    batch_size, seq_len = input_ids.shape

                    # è¿è¡Œå‰å‘ä¼ æ’­ï¼ˆè§¦å‘æ‰€æœ‰hooksï¼‰
                    self.model(input_ids)

                    # ä¿å­˜attention maskå’Œinput_ids
                    if attention_mask is not None:
                        self.attention_masks.append(attention_mask)
                        # åªè®¡æ•°æœ‰æ•ˆtokensï¼ˆä¸åŒ…æ‹¬paddingï¼‰
                        valid_tokens = int(attention_mask.sum())
                        self.tokens_collected += valid_tokens
                    else:
                        # æ²¡æœ‰maskï¼Œè®¡æ•°æ‰€æœ‰tokens
                        self.tokens_collected += batch_size * seq_len

                    # ä¿å­˜input_idsç”¨äºè¿‡æ»¤ç‰¹æ®Štokens
                    self.input_ids_list.append(input_ids.cpu().numpy())

                    pbar.set_postfix({"tokens": f"{self.tokens_collected:,}/{self.max_tokens:,}"})

        finally:
            self.remove_hooks()

        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„æ¿€æ´»å€¼å’Œattention masks
        print("\nConcatenating activations...")

        # åˆå¹¶attention masksï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.attention_masks:
            print(f"  Concatenating attention masks...")
            combined_mask = np.concatenate(self.attention_masks, axis=0)  # [total_samples, seq_len]
            print(f"  Attention mask shape: {combined_mask.shape}")
        else:
            combined_mask = None

        # åˆå¹¶input_idsç”¨äºè¿‡æ»¤ç‰¹æ®Štokens
        if self.input_ids_list:
            print(f"  Concatenating input_ids...")
            combined_input_ids = np.concatenate(self.input_ids_list, axis=0)  # [total_samples, seq_len]
            print(f"  Input IDs shape: {combined_input_ids.shape}")
        else:
            combined_input_ids = None

        # å¹¶è¡Œå¤„ç†æ¯ä¸ªhookpointçš„æ¿€æ´»å€¼
        print(f"  Processing {len(self.hookpoints)} hookpoints...")

        for name in self.hookpoints:
            if not self.activations[name]:
                print(f"  âš ï¸  {name}: No activations collected!")
                continue

            # Concatenateæ¿€æ´»å€¼
            acts = np.concatenate(self.activations[name], axis=0)  # [total_samples, seq_len, d_in]

            # åˆ›å»ºè¿‡æ»¤mask
            if combined_mask is not None or combined_input_ids is not None:
                # å±•å¼€activations: [total_samples, seq_len, d_in] -> [total_samples * seq_len, d_in]
                acts_flat = acts.reshape(-1, acts.shape[-1])
                total_tokens = acts_flat.shape[0]

                # åˆå§‹åŒ–è¿‡æ»¤maskï¼ˆå…¨Trueè¡¨ç¤ºä¿ç•™æ‰€æœ‰ï¼‰
                filter_mask = np.ones(total_tokens, dtype=bool)

                # è¿‡æ»¤padding tokens
                if combined_mask is not None:
                    mask_flat = combined_mask.reshape(-1)
                    filter_mask &= (mask_flat == 1)

                # è¿‡æ»¤ç‰¹æ®Štokens (BOS, EOS, etc.)
                if combined_input_ids is not None and self.special_token_ids:
                    input_ids_flat = combined_input_ids.reshape(-1)
                    # æ ‡è®°ä¸æ˜¯ç‰¹æ®Štokençš„ä½ç½®
                    not_special = ~np.isin(input_ids_flat, list(self.special_token_ids))
                    filter_mask &= not_special

                # åº”ç”¨è¿‡æ»¤
                acts_filtered = acts_flat[filter_mask]

                # ç»Ÿè®¡
                valid_tokens = acts_filtered.shape[0]
                removed_tokens = total_tokens - valid_tokens
                padding_count = np.sum(~(combined_mask.reshape(-1) == 1)) if combined_mask is not None else 0
                special_count = removed_tokens - padding_count

                self.activations[name] = acts_filtered
                if special_count > 0:
                    print(f"  {name}: {acts_filtered.shape} (removed {padding_count:,} padding + {special_count:,} special tokens)")
                else:
                    print(f"  {name}: {acts_filtered.shape} (removed {padding_count:,} padding tokens)")
            else:
                self.activations[name] = acts
                print(f"  {name}: {acts.shape}")

        return dict(self.activations)


def main():
    parser = argparse.ArgumentParser(description="Compute elbow thresholds for model activations")
    parser.add_argument("model", type=str, help="Model name or path")
    parser.add_argument("--dataset", type=str, default="togethercomputer/RedPajama-Data-1T-Sample",
                        help="Dataset name or path")
    parser.add_argument("--hookpoints", nargs="+", required=True,
                        help='Hookpoints to collect (supports range syntax like "layers.[0-10].self_attn.o_proj")')
    parser.add_argument("--num_tokens", type=int, default=10_000_000,
                        help="Number of tokens to collect")
    parser.add_argument("--batch_size", type=int, default=8,
                        help="Batch size for data loading")
    parser.add_argument("--ctx_len", type=int, default=2048,
                        help="Context length / sequence length for memmap datasets")
    parser.add_argument("--output", type=str, default="thresholds.json",
                        help="Output JSON file")
    parser.add_argument("--max_percentile", type=float, default=0.95,
                        help="Max percentile for kneedle algorithm")
    parser.add_argument("--plot_dir", type=str, default=None,
                        help="Directory to save elbow curve plots (optional)")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu",
                        help="Device to use")

    args = parser.parse_args()

    print(f"ğŸš€ Loading model: {args.model}")
    model = AutoModelForCausalLM.from_pretrained(
        args.model,
        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32,
        device_map=args.device,
    )
    model.eval()

    # Load tokenizer to get special token IDs
    print(f"ğŸ”‘ Loading tokenizer to identify special tokens...")
    tokenizer = AutoTokenizer.from_pretrained(args.model)

    # Collect all special token IDs
    special_token_ids = set()
    if tokenizer.bos_token_id is not None:
        special_token_ids.add(tokenizer.bos_token_id)
    if tokenizer.eos_token_id is not None:
        special_token_ids.add(tokenizer.eos_token_id)
    if tokenizer.pad_token_id is not None:
        special_token_ids.add(tokenizer.pad_token_id)
    if tokenizer.unk_token_id is not None:
        special_token_ids.add(tokenizer.unk_token_id)
    # Add any additional special tokens
    if hasattr(tokenizer, 'additional_special_tokens_ids'):
        special_token_ids.update(tokenizer.additional_special_tokens_ids)

    print(f"   Found {len(special_token_ids)} special token IDs: {sorted(special_token_ids)}")

    print(f"ğŸ“Š Loading dataset: {args.dataset}")

    # Try different dataset loading strategies
    dataset = None
    dataset_type = None

    # Strategy 1: Try memmap dataset (sparsify format)
    try:
        dataset = MemmapDataset(args.dataset, ctx_len=args.ctx_len, max_examples=None)
        dataset_type = "memmap"
        print(f"   âœ… Using memmap dataset: {len(dataset)} examples (ctx_len={args.ctx_len})")
    except (FileNotFoundError, ValueError, OSError, TypeError) as e:
        print(f"   â„¹ï¸  Not a memmap dataset, trying HuggingFace...")

    # Strategy 2: Try HuggingFace dataset
    if dataset is None:
        from datasets import load_dataset

        try:
            # Estimate how many examples we need
            # Assuming avg sequence length ~= ctx_len
            estimated_examples = max(
                int(args.num_tokens / args.ctx_len * 2),  # 2x buffer for safety
                1000  # minimum 1k examples
            )
            print(f"   â„¹ï¸  Estimated examples needed: {estimated_examples:,} (for {args.num_tokens:,} tokens)")

            # Load dataset with limit (use streaming if dataset is large)
            print(f"   ğŸ”„ Loading HuggingFace dataset (streaming mode)...")
            hf_dataset = load_dataset(args.dataset, split="train", streaming=True)

            # Take only what we need
            hf_dataset = hf_dataset.take(estimated_examples)
            print(f"   âœ… Will process ~{estimated_examples:,} examples")

            # Check if already tokenized (sample first element)
            first_example = next(iter(hf_dataset))
            if "input_ids" in first_example:
                print(f"   âœ… Dataset already tokenized")
                dataset = hf_dataset
                dataset_type = "hf_tokenized"
            else:
                # Need to tokenize (tokenizer already loaded above)
                print(f"   â„¹ï¸  Dataset not tokenized, tokenizing...")

                # Detect text column
                text_column = None
                for col in ["text", "content", "document", "article"]:
                    if col in first_example:
                        text_column = col
                        break

                if text_column is None:
                    raise ValueError(
                        f"Could not find text column in dataset. "
                        f"Available columns: {list(first_example.keys())}"
                    )

                print(f"   âœ… Found text column: '{text_column}'")

                def tokenize_function(examples):
                    return tokenizer(
                        examples[text_column],
                        truncation=True,
                        max_length=args.ctx_len,
                        return_tensors=None,
                    )

                print(f"   ğŸ”„ Tokenizing ~{estimated_examples:,} examples...")
                dataset = hf_dataset.map(
                    tokenize_function,
                    batched=True,
                    remove_columns=list(first_example.keys()),
                )
                dataset_type = "hf_raw"
                print(f"   âœ… Tokenization complete")

        except Exception as e:
            print(f"   âŒ Failed to load dataset: {e}")
            raise

    # Create dataloader
    from torch.utils.data import DataLoader

    def collate_fn(examples):
        """Collate function that handles different input formats"""
        input_ids_list = []
        for ex in examples:
            if isinstance(ex, dict):
                ids = ex.get("input_ids", ex.get("tokens", None))
            else:
                ids = ex

            if ids is None:
                raise ValueError(f"Could not find input_ids in example: {ex.keys() if isinstance(ex, dict) else type(ex)}")

            # Convert to tensor if needed
            if not isinstance(ids, torch.Tensor):
                ids = torch.tensor(ids)

            input_ids_list.append(ids)

        # Pad all sequences to fixed ctx_len (not just batch max)
        target_len = args.ctx_len

        # Pad each sequence and create attention mask
        padded_ids = []
        attention_mask = []
        for ids in input_ids_list:
            seq_len = ids.shape[0]

            # Truncate if too long
            if seq_len > target_len:
                ids = ids[:target_len]
                seq_len = target_len

            # Create attention mask (1 for real tokens, 0 for padding)
            mask = torch.ones(seq_len, dtype=torch.long)

            if seq_len < target_len:
                # Pad with 0
                padding = torch.zeros(target_len - seq_len, dtype=ids.dtype)
                ids = torch.cat([ids, padding])
                # Extend mask with 0s for padding
                padding_mask = torch.zeros(target_len - seq_len, dtype=torch.long)
                mask = torch.cat([mask, padding_mask])

            padded_ids.append(ids)
            attention_mask.append(mask)

        # Stack into batch
        return {
            "input_ids": torch.stack(padded_ids),
            "attention_mask": torch.stack(attention_mask)
        }

    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        collate_fn=collate_fn,
        num_workers=0,  # Use 0 for better compatibility
    )

    # æ‰©å±•èŒƒå›´æ¨¡å¼å¹¶åŒ¹é…å®é™…çš„module names
    print(f"\nğŸ” Expanding hookpoint patterns...")
    expanded_patterns = []
    for pattern in args.hookpoints:
        expanded = expand_range_pattern(pattern)
        expanded_patterns.extend(expanded)
        if len(expanded) > 1:
            print(f"   {pattern} â†’ {len(expanded)} patterns")

    # åŒ¹é…å®é™…çš„module names
    matched_hookpoints = []
    for name, _ in model.base_model.named_modules():  # ä½¿ç”¨ base_modelï¼Œå’Œtrainer.pyä¸€è‡´
        if any(fnmatchcase(name, pat) for pat in expanded_patterns):
            matched_hookpoints.append(name)

    matched_hookpoints = natsorted(matched_hookpoints)
    print(f"\nâœ… Matched {len(matched_hookpoints)} hookpoints:")
    for hp in matched_hookpoints:
        print(f"   - {hp}")

    if not matched_hookpoints:
        print("âŒ No hookpoints matched! Check your patterns.")
        return

    # æ”¶é›†æ¿€æ´»å€¼
    print(f"\nğŸ“¥ Collecting activations (target: {args.num_tokens:,} tokens)...")
    collector = ActivationCollector(model, matched_hookpoints, args.num_tokens, special_token_ids)
    activations = collector.collect(dataloader, args.device)

    # è®¡ç®—elbowé˜ˆå€¼ï¼ˆå¹¶è¡Œï¼‰
    print(f"\nğŸ§® Computing elbow thresholds (parallel)...")

    # è¿‡æ»¤æ‰ç©ºæ•°æ®
    valid_activations = {name: acts for name, acts in activations.items() if len(acts) > 0}

    if not valid_activations:
        print("âŒ No valid activation data collected!")
        return

    # åˆ›å»ºplotç›®å½•ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    plot_dir_str = None
    if args.plot_dir:
        plot_dir = Path(args.plot_dir)
        plot_dir.mkdir(parents=True, exist_ok=True)
        plot_dir_str = str(plot_dir)  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç”¨äºmultiprocessing
        print(f"   ğŸ“Š Plots will be saved to: {plot_dir}")

    # å¹¶è¡Œè®¡ç®—
    num_workers = min(len(valid_activations), cpu_count())
    print(f"   Using {num_workers} workers for {len(valid_activations)} hookpoints")

    thresholds = {}

    # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œè®¡ç®—
    with Pool(processes=num_workers) as pool:
        # åˆ›å»ºéƒ¨åˆ†åº”ç”¨å‡½æ•°
        compute_func = partial(compute_elbow_for_layer,
                              max_percentile=args.max_percentile,
                              plot_dir=plot_dir_str)  # ä¼ é€’å­—ç¬¦ä¸²è€Œä¸æ˜¯Pathå¯¹è±¡

        # å¹¶è¡Œè®¡ç®—å¹¶æ˜¾ç¤ºè¿›åº¦æ¡
        results = list(tqdm(
            pool.imap(compute_func, valid_activations.items()),
            total=len(valid_activations),
            desc="Computing elbows"
        ))

    # å¤„ç†ç»“æœ
    for name, result, error in results:
        if error is not None:
            print(f"   âŒ {name}: {error}")
            continue

        # è½¬æ¢åç§°æ ¼å¼ï¼šlayers.0.self_attn.o_proj â†’ layer_0/self_attn_o_proj
        if name.startswith("layers."):
            parts = name.split('.', 2)  # ['layers', '0', 'self_attn.o_proj']
            layer_idx = parts[1]
            operator = parts[2].replace('.', '_')
            key = f"layer_{layer_idx}/{operator}"
        else:
            # å…œåº•ï¼šå¦‚æœä¸æ˜¯æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥è½¬æ¢
            key = name.replace(".", "_")

        thresholds[key] = result
        print(f"   âœ… {name}: elbow_p={result['elbow_p']:.4f}, elbow_value={result['elbow_value']:.6f}")

    # ä¿å­˜ç»“æœ
    output_path = Path(args.output)
    print(f"\nğŸ’¾ Saving thresholds to {output_path}")
    with open(output_path, "w") as f:
        json.dump(thresholds, f, indent=2)

    print(f"\nâœ¨ Done! Collected {collector.tokens_collected:,} tokens")
    print(f"   Results saved to: {output_path.absolute()}")


if __name__ == "__main__":
    main()
