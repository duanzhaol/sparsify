# MoE-LowRank SAE Encoder 设计方案

本文档描述一种基于 MoE（Mixture of Experts）和低秩分解的 SAE Encoder 架构，旨在大幅降低在线推理时的访存量，同时保持与标准 SAE 兼容的输出格式。

---

## 目录

1. [设计背景与目标](#1-设计背景与目标)
2. [架构设计](#2-架构设计)
3. [推荐配置](#3-推荐配置)
4. [Router 设计](#4-router-设计)
5. [训练策略：两阶段蒸馏](#5-训练策略两阶段蒸馏)
6. [与现有 Decoder 的兼容性](#6-与现有-decoder-的兼容性)
7. [评估指标](#7-评估指标)
8. [总结](#8-总结)

---

## 1. 设计背景与目标

### 1.1 问题分析

标准 SAE Encoder 在推理时需要加载完整的编码器权重矩阵，访存量巨大：

```
标准 SAE Encoder 访存量计算：
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
假设：hidden_size (H) = 4096, expansion_factor = 8
      num_latents (M) = 32768

W_enc: [M, H] = [32768, 4096]
访存量 = 32768 × 4096 × 2 bytes (BF16) = 256 MB

问题：这可能比大模型自身的某些权重矩阵还要大！
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 1.2 设计目标

| 约束项 | 要求 |
|--------|------|
| 访存量 | ≤ 原始的 2% (≤ 5.12 MB) |
| 输出格式 | 与标准 SAE 兼容：`(top_indices, top_acts)` |
| Decoder | 保持不变，直接复用标准 SAE 的 decoder |
| 重构精度 | FVU 相比标准 SAE 劣化 ≤ 10% |

### 1.3 核心思路

借鉴现代 LLM 中的稀疏 MoE 架构：
- 将 encoder 分成多个**专家（Expert）**
- 每个 token 只**激活少量专家**，大幅减少访存
- 每个专家内部使用**低秩分解**，进一步压缩参数量

---

## 2. 架构设计

### 2.1 整体架构

```
┌─────────────────────────────────────────────────────────────────────────┐
│                      MoE-LowRank SAE Encoder                            │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  输入: x ∈ ℝᴴ  (单个 token 的 hidden state)                             │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                     Stage 1: Expert Routing                      │   │
│  │                                                                  │   │
│  │   router_logits = x @ W_router + b_router                       │   │
│  │                   [H] @ [H, E] → [E]                            │   │
│  │                                                                  │   │
│  │   expert_indices = TopK(router_logits, e)     → 选择 e 个专家   │   │
│  │   expert_weights = Softmax(router_logits[expert_indices])       │   │
│  │                                                                  │   │
│  │   访存: W_router [H, E] ≈ 0.5-2 MB                              │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                              │                                          │
│                              ▼                                          │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                  Stage 2: Low-Rank Expert Encoding               │   │
│  │                                                                  │   │
│  │   对每个被选中的专家 i ∈ expert_indices:                         │   │
│  │                                                                  │   │
│  │   ┌─────────────────────────────────────────────────────────┐   │   │
│  │   │  Expert i (Low-Rank Decomposition)                      │   │   │
│  │   │                                                         │   │   │
│  │   │  hidden_i = x @ Bᵢᵀ           [H] @ [H, r] → [r]       │   │   │
│  │   │  pre_acts_i = hidden_i @ Aᵢᵀ + bᵢ   [r] @ [r, L] → [L] │   │   │
│  │   │  acts_i = ReLU(pre_acts_i)                              │   │   │
│  │   │                                                         │   │   │
│  │   │  Aᵢ: [L, r]  (latent 方向)                              │   │   │
│  │   │  Bᵢ: [r, H]  (input 投影)                               │   │   │
│  │   │  bᵢ: [L]     (bias)                                     │   │   │
│  │   └─────────────────────────────────────────────────────────┘   │   │
│  │                                                                  │   │
│  │   每个专家访存: (L×r + r×H + L) × 2 bytes                       │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                              │                                          │
│                              ▼                                          │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                    Stage 3: Global TopK Selection                │   │
│  │                                                                  │   │
│  │   1. 收集所有专家的激活值:                                       │   │
│  │      all_acts = Concat([acts_i × expert_weights[i] for i])      │   │
│  │      all_indices = Concat([offset_i + local_indices for i])     │   │
│  │                                                                  │   │
│  │   2. 全局 TopK 选择:                                             │   │
│  │      top_acts, top_idx = TopK(all_acts, k)                      │   │
│  │      top_indices = all_indices[top_idx]                         │   │
│  │                                                                  │   │
│  │   输出: (top_indices, top_acts) — 与标准 SAE 格式完全一致        │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 2.2 数学形式化

**标准 SAE Encoder：**
```
pre_acts = ReLU(x @ W_encᵀ + b_enc)      W_enc: [M, H]
top_acts, top_indices = TopK(pre_acts, k)
```

**MoE-LowRank Encoder：**
```
# Stage 1: Routing
expert_indices = TopK(x @ W_router, e)   W_router: [H, E]

# Stage 2: Per-expert encoding (低秩分解)
for i in expert_indices:
    # 原始: x @ Wᵢᵀ ≈ x @ (Bᵢᵀ @ Aᵢᵀ) = (x @ Bᵢᵀ) @ Aᵢᵀ
    pre_acts_i = ReLU((x @ Bᵢᵀ) @ Aᵢᵀ + bᵢ)

# Stage 3: Global TopK
top_acts, top_indices = GlobalTopK(all_pre_acts, k)
```

### 2.3 参数命名与维度

| 参数 | 维度 | 说明 |
|------|------|------|
| `W_router` | `[H, E]` | Router 权重 |
| `b_router` | `[E]` | Router 偏置 |
| `experts.A` | `[E, L, r]` | 专家的 latent 投影矩阵 |
| `experts.B` | `[E, r, H]` | 专家的 input 投影矩阵 |
| `experts.bias` | `[E, L]` | 专家的偏置 |

**符号说明：**
- `E` = 专家总数
- `e` = 每个 token 激活的专家数
- `L` = 每个专家负责的 latent 数量 (E × L = M)
- `r` = 低秩分解的 rank
- `M` = 总 latent 数量 (= H × expansion_factor)
- `H` = hidden_size

### 2.4 访存量计算公式

```
总访存量 = Router访存 + 激活专家访存

Router访存 = H × E × 2 bytes

单个专家访存 = (L × r + r × H + L) × 2 bytes
            ≈ (L × r + r × H) × 2 bytes  (忽略 bias)

激活专家访存 = e × 单个专家访存

总访存量 = H × E × 2 + e × (L × r + r × H) × 2
```

---

## 3. 推荐配置

### 3.1 三种配置方案

基于 H=4096, M=32768 (expansion=8), k=32：

```
┌──────────────────────────────────────────────────────────────────────────┐
│                          配置方案对比                                      │
├────────────┬────────────┬────────────┬────────────┬─────────────────────┤
│   参数      │  配置 A    │  配置 B    │  配置 C    │       说明          │
│            │  (激进)    │  (均衡)    │  (保守)    │                     │
├────────────┼────────────┼────────────┼────────────┼─────────────────────┤
│ E (专家数)  │    64     │    128    │    256    │ 更多专家=更细粒度    │
│ e (激活数)  │     2     │     4     │     8     │ 更多激活=更好覆盖    │
│ L (latent/专家)│  512    │    256    │    128    │ E × L = 32768      │
│ r (rank)   │    64     │    64     │    64     │ 低秩分解维度         │
├────────────┼────────────┼────────────┼────────────┼─────────────────────┤
│ Router     │  0.50 MB  │  1.00 MB  │  2.00 MB  │ H × E × 2          │
│ 单专家      │  0.56 MB  │  0.53 MB  │  0.52 MB  │ (L×r + r×H) × 2    │
│ 总访存      │  1.62 MB  │  3.12 MB  │  6.16 MB  │ Router + e × Expert │
├────────────┼────────────┼────────────┼────────────┼─────────────────────┤
│ 压缩比      │  0.63%   │  1.22%   │  2.41%   │ vs 256MB           │
│ 满足约束?   │    ✓     │    ✓     │    ✗     │ 目标 ≤ 2%          │
└────────────┴────────────┴────────────┴────────────┴─────────────────────┘
```

### 3.2 配置调整策略

如果配置 C 想满足 2% 约束，可以采用以下调整：

| 调整方案 | 参数变化 | 结果访存 | 压缩比 |
|----------|----------|----------|--------|
| 降低 rank | r: 64→48 | 4.6 MB | 1.8% ✓ |
| 降低激活专家数 | e: 8→6 | 5.1 MB | 2.0% ✓ |
| 两者结合 | r=56, e=7 | 4.9 MB | 1.9% ✓ |

### 3.3 推荐实验顺序

```
建议按以下顺序实验，逐步调整复杂度：

实验 1: 配置 B (均衡方案) ← 推荐首选
        E=128, e=4, L=256, r=64
        访存 3.12 MB (1.22%)
        ↓
        如果精度不够 ↓

实验 2: 增加激活专家
        E=128, e=6, L=256, r=64
        访存 4.18 MB (1.63%)
        ↓
        如果还不够 ↓

实验 3: 增加 rank
        E=128, e=6, L=256, r=96
        访存 5.35 MB (2.09%) - 略超但可接受
        ↓
        如果访存太大 ↓

实验 4: 配置 A (激进方案)
        E=64, e=2, L=512, r=64
        访存 1.62 MB (0.63%)
        作为下限参考
```

---

## 4. Router 设计

### 4.1 Learned Router 结构

```
┌─────────────────────────────────────────────────────────────────┐
│                      Learned Router 设计                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  结构：简单线性层 + TopK                                          │
│                                                                  │
│  router_logits = x @ W_router + b_router    [H] → [E]           │
│  expert_indices = TopK(router_logits, e)    选择 e 个专家        │
│  expert_weights = Softmax(TopK_values)      归一化权重           │
│                                                                  │
│  优点：                                                          │
│  - 可以学习 input → expert 的语义映射                            │
│  - 端到端可微分                                                  │
│  - 实现简单，推理高效                                            │
│                                                                  │
│  访存开销：                                                       │
│  - W_router: [H, E] = [4096, 128] × 2 = 1 MB                   │
│  - 相比专家权重，router 开销很小                                  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### 4.2 负载均衡设计

Router 的一个关键问题是**专家负载不均衡**（某些专家被过度使用，某些专家从不激活）：

```
┌─────────────────────────────────────────────────────────────────┐
│                      负载均衡策略                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  方法 1: Auxiliary Load Balancing Loss (推荐)                    │
│  ════════════════════════════════════════════                    │
│                                                                  │
│  定义：                                                          │
│  - fᵢ = 专家 i 被选中的频率 (batch 内统计)                       │
│  - pᵢ = 专家 i 的平均 routing 概率                               │
│                                                                  │
│  L_balance = α × E × Σᵢ (fᵢ × pᵢ)                               │
│                                                                  │
│  理想情况：所有专家均匀使用，fᵢ = pᵢ = 1/E                       │
│  α 通常取 0.01 ~ 0.1                                            │
│                                                                  │
│  ─────────────────────────────────────────────────────────────  │
│                                                                  │
│  方法 2: Expert Capacity (可选，用于 batch 推理)                  │
│  ════════════════════════════════════════════                    │
│                                                                  │
│  限制每个专家最多处理的 token 数量：                              │
│  capacity = (batch_size × e) / E × buffer_factor               │
│                                                                  │
│  超出容量的 token 被丢弃或路由到其他专家                          │
│                                                                  │
│  注意：单 token 推理时不需要此机制                                │
│                                                                  │
│  ─────────────────────────────────────────────────────────────  │
│                                                                  │
│  方法 3: Router Z-Loss (可选，防止 logits 过大)                   │
│  ════════════════════════════════════════════                    │
│                                                                  │
│  L_z = β × mean(log(Σᵢ exp(router_logits)))²                    │
│  β 通常取 0.001                                                  │
│                                                                  │
│  作用：防止 router logits 数值爆炸，提高训练稳定性                │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### 4.3 Router 初始化策略

```
┌─────────────────────────────────────────────────────────────────┐
│                   Router 初始化策略                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  方案 A: 随机初始化（简单）                                       │
│  ────────────────────────                                        │
│  W_router ~ N(0, 1/√H)                                          │
│  b_router = 0                                                   │
│                                                                  │
│  适用场景：从头训练，或快速实验                                   │
│                                                                  │
│  ─────────────────────────────────────────────────────────────  │
│                                                                  │
│  方案 B: 基于标准 SAE 的聚类初始化（推荐）                         │
│  ────────────────────────────────────────                        │
│                                                                  │
│  步骤：                                                          │
│  1. 训练标准 SAE，获得 W_enc: [M, H]                             │
│  2. 对 W_enc 的行进行 K-means 聚类，得到 E 个聚类中心             │
│  3. 将聚类中心作为 W_router 的列向量                              │
│                                                                  │
│  原理：                                                          │
│  - 聚类中心代表了不同 latent 组的"典型方向"                       │
│  - 当 input 与某方向相似时，应该路由到对应的专家组                │
│  - 这样初始 router 就具有语义意义                                 │
│                                                                  │
│  ─────────────────────────────────────────────────────────────  │
│                                                                  │
│  方案 C: 基于专家平均方向初始化                                   │
│  ──────────────────────────────                                  │
│                                                                  │
│  W_router[:, i] = mean(W_enc[专家i的latent], dim=0)             │
│  W_router = normalize(W_router, dim=0)                          │
│                                                                  │
│  直觉：使用每个专家负责的 latent 的平均方向作为路由依据           │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 5. 训练策略：两阶段蒸馏

### 5.1 整体流程

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         两阶段训练流程                                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ╔═══════════════════════════════════════════════════════════════════╗ │
│  ║  阶段 1: 训练标准 SAE (Teacher)                                    ║ │
│  ╠═══════════════════════════════════════════════════════════════════╣ │
│  ║                                                                    ║ │
│  ║  使用现有 sparsify 代码训练标准 SAE                                ║ │
│  ║                                                                    ║ │
│  ║  输出:                                                             ║ │
│  ║  - W_enc: [M, H]        (encoder 权重，用于初始化和蒸馏)           ║ │
│  ║  - W_dec: [M, H]        (decoder 权重，直接复用)                   ║ │
│  ║  - b_enc: [M]           (encoder bias)                            ║ │
│  ║  - b_dec: [H]           (decoder bias)                            ║ │
│  ║                                                                    ║ │
│  ╚═══════════════════════════════════════════════════════════════════╝ │
│                              │                                          │
│                              ▼                                          │
│  ╔═══════════════════════════════════════════════════════════════════╗ │
│  ║  阶段 2: 训练 MoE-LowRank Encoder (Student)                        ║ │
│  ╠═══════════════════════════════════════════════════════════════════╣ │
│  ║                                                                    ║ │
│  ║  2a. 初始化 (利用 Teacher 的知识)                                  ║ │
│  ║                                                                    ║ │
│  ║  2b. 蒸馏训练                                                      ║ │
│  ║      - 固定 W_dec（从阶段1复用）                                   ║ │
│  ║      - 只训练 MoE encoder 参数                                     ║ │
│  ║                                                                    ║ │
│  ║  2c. (可选) 联合微调                                               ║ │
│  ║      - 解冻 W_dec，小学习率微调                                    ║ │
│  ║                                                                    ║ │
│  ╚═══════════════════════════════════════════════════════════════════╝ │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 5.2 阶段 2a: MoE Encoder 初始化

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    MoE Encoder 初始化策略                                │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  目标：利用标准 SAE 的知识来初始化 MoE encoder，加速收敛                │
│                                                                         │
│  ═══════════════════════════════════════════════════════════════════   │
│  Step 1: 将 W_enc 分配给各专家                                          │
│  ═══════════════════════════════════════════════════════════════════   │
│                                                                         │
│  方法 A: 顺序分割（简单）                                               │
│  ─────────────────────                                                  │
│  Expert i 负责 latent [i×L : (i+1)×L]                                  │
│  W_enc_i = W_enc[i×L : (i+1)×L, :]    shape: [L, H]                   │
│                                                                         │
│  优点：实现简单                                                         │
│  缺点：相邻 latent 可能语义不相关，低秩近似效果差                        │
│                                                                         │
│  方法 B: 基于相似性聚类（推荐）                                         │
│  ─────────────────────────────                                          │
│  1. 对 W_enc 的行做 K-means 聚类，分成 E 个簇                          │
│  2. 每个簇的 latent 分配给对应的专家                                    │
│  3. 同一专家内的 latent 语义更相关，低秩近似效果更好                    │
│                                                                         │
│  优点：语义相关的 latent 聚在一起，低秩分解损失更小                      │
│  缺点：需要额外的聚类步骤                                               │
│                                                                         │
│  ═══════════════════════════════════════════════════════════════════   │
│  Step 2: 对每个专家做低秩分解 (SVD)                                     │
│  ═══════════════════════════════════════════════════════════════════   │
│                                                                         │
│  对 W_enc_i ∈ ℝ^{L×H} 做 SVD 分解:                                     │
│                                                                         │
│  W_enc_i = U @ Σ @ Vᵀ                                                  │
│                                                                         │
│  取前 r 个奇异值:                                                       │
│  Aᵢ = U[:, :r] @ √Σ[:r, :r]     shape: [L, r]                         │
│  Bᵢ = √Σ[:r, :r] @ V[:, :r]ᵀ    shape: [r, H]                         │
│                                                                         │
│  验证: Aᵢ @ Bᵢ ≈ W_enc_i                                               │
│                                                                         │
│  说明：                                                                 │
│  - SVD 给出最优的低秩近似（Eckart-Young 定理）                          │
│  - 将奇异值平方根分配给 A 和 B，使两者的范数更均衡                      │
│                                                                         │
│  ═══════════════════════════════════════════════════════════════════   │
│  Step 3: 初始化 Router                                                  │
│  ═══════════════════════════════════════════════════════════════════   │
│                                                                         │
│  方法: 使用每个专家的 latent 平均方向作为 router 权重                    │
│                                                                         │
│  W_router[:, i] = mean(W_enc_i, dim=0)   # 专家 i 的平均方向            │
│  W_router = normalize(W_router, dim=0)   # 归一化                       │
│                                                                         │
│  直觉：当 input 与某专家的 latent 方向相似时，应路由到该专家            │
│                                                                         │
│  ═══════════════════════════════════════════════════════════════════   │
│  Step 4: 初始化 Bias                                                    │
│  ═══════════════════════════════════════════════════════════════════   │
│                                                                         │
│  experts.bias[i] = b_enc[专家i的latent索引]                             │
│                                                                         │
│  直接复用标准 SAE 的 encoder bias                                       │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 5.3 阶段 2b: 蒸馏损失设计

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         蒸馏损失函数                                     │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  总损失:                                                                │
│  L_total = L_recon + λ₁ × L_distill + λ₂ × L_balance + λ₃ × L_auxk    │
│                                                                         │
│  ═══════════════════════════════════════════════════════════════════   │
│  1. 重构损失 L_recon (主损失)                                           │
│  ═══════════════════════════════════════════════════════════════════   │
│                                                                         │
│  与标准 SAE 相同，使用 FVU:                                             │
│                                                                         │
│  sae_out = decode(top_acts, top_indices)    # 使用冻结的 W_dec         │
│  L_recon = ||x - sae_out||² / Var(x)                                   │
│                                                                         │
│  ═══════════════════════════════════════════════════════════════════   │
│  2. 蒸馏损失 L_distill (关键！)                                         │
│  ═══════════════════════════════════════════════════════════════════   │
│                                                                         │
│  让 MoE encoder 的输出模仿 Teacher (标准 SAE) 的输出:                   │
│                                                                         │
│  方法 A: Output Distillation（推荐，简单有效）                          │
│  ─────────────────────────────────────────────                          │
│  teacher_acts, teacher_indices = TeacherSAE.encode(x)                  │
│  student_acts, student_indices = MoEEncoder(x)                         │
│                                                                         │
│  # 比较解码后的输出                                                     │
│  teacher_out = decode(teacher_acts, teacher_indices)                   │
│  student_out = decode(student_acts, student_indices)                   │
│  L_distill = ||teacher_out - student_out||² / Var(x)                   │
│                                                                         │
│  方法 B: Activation Distillation（更精细，可选）                        │
│  ───────────────────────────────────────────────                        │
│  # 比较 top-k 激活值的分布                                              │
│  L_distill = KL(Softmax(student_acts) || Softmax(teacher_acts))        │
│            + λ × IndexMatchLoss(student_indices, teacher_indices)      │
│                                                                         │
│  ═══════════════════════════════════════════════════════════════════   │
│  3. 负载均衡损失 L_balance                                              │
│  ═══════════════════════════════════════════════════════════════════   │
│                                                                         │
│  L_balance = E × Σᵢ (fᵢ × pᵢ)                                          │
│  其中 fᵢ = 专家 i 的使用频率, pᵢ = 平均 routing 概率                   │
│                                                                         │
│  ═══════════════════════════════════════════════════════════════════   │
│  4. AuxK 损失 L_auxk (处理死 latent)                                    │
│  ═══════════════════════════════════════════════════════════════════   │
│                                                                         │
│  复用标准 SAE 的 AuxK 机制，鼓励死 latent 预测残差                      │
│  （详见 sparse_coder.py 中的实现）                                      │
│                                                                         │
│  ═══════════════════════════════════════════════════════════════════   │
│  推荐超参数                                                             │
│  ═══════════════════════════════════════════════════════════════════   │
│                                                                         │
│  λ₁ (distill)  = 0.5 ~ 1.0   # 蒸馏权重，训练初期可以大一些            │
│  λ₂ (balance)  = 0.01 ~ 0.1  # 负载均衡                                │
│  λ₃ (auxk)     = 1/32        # 与标准 SAE 相同                         │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 5.4 训练调度

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         训练调度策略                                     │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  阶段 2 的三个子阶段:                                                   │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ 子阶段 1: Router Warmup (约 5-10% 的总步数)                      │   │
│  │                                                                  │   │
│  │ 训练内容:                                                        │   │
│  │ - 冻结专家权重 (A, B)                                           │   │
│  │ - 只训练 Router (W_router, b_router)                            │   │
│  │                                                                  │   │
│  │ 损失函数: L_distill + L_balance                                 │   │
│  │                                                                  │   │
│  │ 目标: 让 router 学会合理的专家分配，再联合训练专家               │   │
│  │                                                                  │   │
│  │ 学习率: 较大 (如 1e-3)                                          │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                              ↓                                          │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ 子阶段 2: Joint Training (约 80-90% 的总步数)                    │   │
│  │                                                                  │   │
│  │ 训练内容:                                                        │   │
│  │ - 解冻所有 MoE encoder 参数 (Router + Experts)                  │   │
│  │ - W_dec 保持冻结                                                 │   │
│  │                                                                  │   │
│  │ 损失函数: L_recon + λ₁×L_distill + λ₂×L_balance + λ₃×L_auxk    │   │
│  │                                                                  │   │
│  │ 调度策略:                                                        │   │
│  │ - 逐渐降低 λ₁ (蒸馏权重)                                        │   │
│  │ - 初始 λ₁=1.0 → 最终 λ₁=0.1                                     │   │
│  │ - 让模型逐渐从"模仿 Teacher"过渡到"独立重构"                     │   │
│  │                                                                  │   │
│  │ 学习率: 中等 (如 5e-4)，使用 cosine decay                       │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                              ↓                                          │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ 子阶段 3: Fine-tuning (可选, 约 5% 的总步数)                     │   │
│  │                                                                  │   │
│  │ 训练内容:                                                        │   │
│  │ - 解冻 W_dec                                                     │   │
│  │ - 小学习率联合微调所有参数                                       │   │
│  │                                                                  │   │
│  │ 损失函数: L_recon + L_auxk (去掉蒸馏损失)                       │   │
│  │                                                                  │   │
│  │ 学习率: 很小 (如 1e-5)                                          │   │
│  │                                                                  │   │
│  │ 注意: 如果子阶段2的精度已经足够，可以跳过此阶段                  │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 6. 与现有 Decoder 的兼容性

### 6.1 输出接口设计

```
┌─────────────────────────────────────────────────────────────────────────┐
│                      接口兼容性设计                                       │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  标准 SAE 的 encode() 输出:                                             │
│  ─────────────────────────                                              │
│  top_acts: [batch, k]      # k 个激活值                                 │
│  top_indices: [batch, k]   # k 个全局索引 ∈ [0, M)                      │
│                                                                         │
│  MoE Encoder 的输出 (必须保持一致):                                      │
│  ─────────────────────────────────                                      │
│  top_acts: [batch, k]      # k 个激活值                                 │
│  top_indices: [batch, k]   # k 个全局索引 ∈ [0, M)                      │
│                                                                         │
│  ═══════════════════════════════════════════════════════════════════   │
│  关键：top_indices 必须是全局索引，不是专家内的局部索引                  │
│  ═══════════════════════════════════════════════════════════════════   │
│                                                                         │
│  索引转换方法:                                                          │
│  ─────────────                                                          │
│                                                                         │
│  方法 A: 顺序分割时                                                     │
│  专家 i 的局部索引 local_idx ∈ [0, L)                                   │
│  全局索引 = i × L + local_idx                                          │
│                                                                         │
│  方法 B: 聚类分配时                                                     │
│  需要维护一个映射表 latent_to_global: [E, L] → [M]                      │
│  全局索引 = latent_to_global[expert_id, local_idx]                     │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 6.2 完整推理流程

```
┌─────────────────────────────────────────────────────────────────────────┐
│                      推理时的完整流程                                     │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  # 输入: 单个 token 的 hidden state                                     │
│  x: [H]                                                                 │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ Step 1: MoE Encoder (新设计，低访存)                             │   │
│  │                                                                  │   │
│  │ top_acts, top_indices = moe_encoder.encode(x)                   │   │
│  │                                                                  │   │
│  │ 访存量: ~3 MB (相比原始 256 MB)                                  │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                              │                                          │
│                              ▼                                          │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ Step 2: 使用 top_indices 进行查表 (用户的加速方案)               │   │
│  │                                                                  │   │
│  │ # top_indices 指示了哪些 latent 被激活                           │   │
│  │ # 可以用于查表、稀疏计算等下游任务                               │   │
│  │                                                                  │   │
│  │ result = lookup_table[top_indices]  # 示例                       │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                              │                                          │
│                              ▼                                          │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ Step 3: 如果需要重构 (通常不需要在推理时)                        │   │
│  │                                                                  │   │
│  │ # 可以复用标准 SAE 的 decoder                                    │   │
│  │ reconstructed = standard_sae.decode(top_acts, top_indices)      │   │
│  │                                                                  │   │
│  │ # 因为 MoE encoder 输出格式与标准 SAE 完全兼容                   │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 7. 评估指标

### 7.1 训练时监控指标

| 指标 | 计算方法 | 目标值 | 说明 |
|------|----------|--------|------|
| FVU | `‖x - sae_out‖² / Var(x)` | ≤ Teacher × 1.1 | 主要精度指标 |
| Distill Loss | `‖teacher_out - student_out‖²` | 持续下降 | 蒸馏效果 |
| Expert Usage Std | `std(expert_frequencies)` | 尽量小 | 负载均衡程度 |
| Dead Latents % | 从未激活的 latent 比例 | < 10% | 特征利用率 |
| Dead Experts % | 从未被选中的专家比例 | 0% | 专家利用率 |

### 7.2 推理时评估指标

| 指标 | 计算方法 | 目标值 | 说明 |
|------|----------|--------|------|
| 访存量 | Router + e × Expert | ≤ 5.12 MB | 核心约束 |
| Index Recall@k | `\|MoE_indices ∩ Teacher_indices\| / k` | > 70% | 索引一致性 |
| Activation Cosine | `cos(MoE_out, Teacher_out)` | > 0.9 | 输出相似度 |
| 下游任务精度 | 用户的查表方案最终效果 | ≈ 标准SAE | 最终目标 |

### 7.3 消融实验建议

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        消融实验设计                                       │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  1. 专家数量 E 的影响                                                   │
│     E ∈ {32, 64, 128, 256}                                             │
│     固定 e=4, r=64                                                      │
│     观察: FVU vs 访存量 trade-off                                       │
│                                                                         │
│  2. 激活专家数 e 的影响                                                 │
│     e ∈ {1, 2, 4, 8}                                                   │
│     固定 E=128, r=64                                                    │
│     观察: FVU vs 访存量 trade-off                                       │
│                                                                         │
│  3. Rank r 的影响                                                       │
│     r ∈ {32, 64, 96, 128}                                              │
│     固定 E=128, e=4                                                     │
│     观察: 低秩近似的精度损失                                            │
│                                                                         │
│  4. 初始化方法的影响                                                    │
│     比较: 随机初始化 vs SVD初始化 vs 聚类+SVD初始化                     │
│     观察: 收敛速度和最终精度                                            │
│                                                                         │
│  5. 蒸馏损失的影响                                                      │
│     比较: 有蒸馏 vs 无蒸馏 (直接训练)                                   │
│     观察: 是否蒸馏能提升精度                                            │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 8. 总结

### 8.1 设计要点

```
┌─────────────────────────────────────────────────────────────────────────┐
│                            设计要点总结                                   │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  1. 架构: MoE + Low-Rank                                                │
│     ─────────────────────                                               │
│     - E 个专家，每个 token 激活 e 个                                    │
│     - 每个专家内部用低秩分解 (rank=r)                                   │
│     - 访存量: O(H×E + e×(L×r + r×H)) << O(M×H)                         │
│                                                                         │
│  2. Router: Learned linear router                                       │
│     ─────────────────────────────                                       │
│     - 简单高效，可学习语义路由                                          │
│     - 使用负载均衡 loss 防止专家塌缩                                    │
│     - 基于标准 SAE 的聚类结果初始化                                     │
│                                                                         │
│  3. 训练: 两阶段蒸馏                                                    │
│     ─────────────────────                                               │
│     - 阶段1: 训练标准 SAE (Teacher)                                     │
│     - 阶段2: 初始化 + 蒸馏训练 MoE encoder                              │
│     - 关键: 用 SVD 初始化低秩专家，用聚类初始化 router                  │
│                                                                         │
│  4. 兼容性: 输出格式完全兼容                                            │
│     ───────────────────────────                                         │
│     - (top_indices, top_acts) 格式不变                                  │
│     - Decoder 直接复用标准 SAE 的                                       │
│     - 下游查表方案无需修改                                              │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 8.2 推荐配置

```
┌─────────────────────────────────────────────────────────────────────────┐
│                          推荐配置                                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  首选配置 (均衡方案):                                                   │
│  ════════════════════                                                   │
│                                                                         │
│  E = 128        (专家数量)                                              │
│  e = 4          (每 token 激活专家数)                                   │
│  L = 256        (每专家 latent 数，E×L=32768)                          │
│  r = 64         (低秩分解 rank)                                         │
│                                                                         │
│  访存量: 3.12 MB (原始的 1.22%)                                         │
│  ════════════════════════════════                                       │
│                                                                         │
│  满足 ≤2% 的约束，且有足够的表达能力                                    │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 8.3 预期效果

| 指标 | 标准 SAE | MoE-LowRank SAE | 变化 |
|------|----------|-----------------|------|
| Encoder 访存量 | 256 MB | ~3 MB | **↓ 98.8%** |
| FVU | baseline | baseline × 1.05~1.10 | ↑ 5-10% |
| 输出格式 | (indices, acts) | (indices, acts) | **不变** |
| Decoder | W_dec | W_dec | **不变** |

---

## 附录：符号表

| 符号 | 含义 | 典型值 |
|------|------|--------|
| H | hidden_size | 4096 |
| M | num_latents = H × expansion | 32768 |
| E | 专家总数 | 64~256 |
| e | 每 token 激活专家数 | 2~8 |
| L | 每专家 latent 数 = M / E | 128~512 |
| r | 低秩分解 rank | 48~128 |
| k | 最终输出的 top-k | 32 |

---

*文档创建日期: 2026-01-08*
