# Sparsify è®­ç»ƒåŠ é€ŸæŠ€æœ¯è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†è§£æ sparsify ä¸­ç”¨äºå®ç°é«˜é€Ÿè®­ç»ƒçš„æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯ã€‚

---

## ç›®å½•

1. [æ€§èƒ½ä¼˜åŒ–æ¦‚è§ˆ](#1-æ€§èƒ½ä¼˜åŒ–æ¦‚è§ˆ)
2. [è‡ªå®šä¹‰ Triton Kernel](#2-è‡ªå®šä¹‰-triton-kernel)
3. [èåˆç¼–ç å™¨ (Fused Encoder)](#3-èåˆç¼–ç å™¨-fused-encoder)
4. [æ··åˆç²¾åº¦è®­ç»ƒ (BF16)](#4-æ··åˆç²¾åº¦è®­ç»ƒ-bf16)
5. [TensorCore åŠ é€Ÿ](#5-tensorcore-åŠ é€Ÿ)
6. [ç¨€ç–æ¢¯åº¦è®¡ç®—](#6-ç¨€ç–æ¢¯åº¦è®¡ç®—)
7. [å…¶ä»–ä¼˜åŒ–æŠ€æœ¯](#7-å…¶ä»–ä¼˜åŒ–æŠ€æœ¯)
8. [å®Œæ•´å‰å‘åå‘æµç¨‹](#8-å®Œæ•´å‰å‘åå‘æµç¨‹)
9. [æ€§èƒ½åŸºå‡†ä¸è°ƒä¼˜](#9-æ€§èƒ½åŸºå‡†ä¸è°ƒä¼˜)

---

## 1. æ€§èƒ½ä¼˜åŒ–æ¦‚è§ˆ

### 1.1 æ•´ä½“æ¶æ„

Sparsify çš„è®­ç»ƒé€Ÿåº¦ä¼˜åŠ¿æ¥è‡ªå¤šå±‚æ¬¡çš„ä¼˜åŒ–ç­–ç•¥ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯æ ˆ                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  ç¡¬ä»¶å±‚ï¼š                                                    â”‚
â”‚  â”œâ”€â”€ TensorCore åŠ é€Ÿ (TF32)         ~8x matmul åŠ é€Ÿ         â”‚
â”‚  â””â”€â”€ BF16 æ··åˆç²¾åº¦                  ~2x æ•´ä½“åŠ é€Ÿ             â”‚
â”‚                                                             â”‚
â”‚  ç®—æ³•å±‚ï¼š                                                    â”‚
â”‚  â”œâ”€â”€ TopK ç¨€ç–æ¿€æ´»                  åªæœ‰ k/M çš„è®¡ç®—é‡        â”‚
â”‚  â”œâ”€â”€ ç¨€ç–æ¢¯åº¦è®¡ç®—                   æ¢¯åº¦åªæµç» top-k         â”‚
â”‚  â””â”€â”€ å³æ—¶æ¿€æ´»è®¡ç®—                   æ—  I/O ç“¶é¢ˆ              â”‚
â”‚                                                             â”‚
â”‚  å®ç°å±‚ï¼š                                                    â”‚
â”‚  â”œâ”€â”€ Triton è‡ªå®šä¹‰ Kernel           ç¨€ç–è§£ç ä¼˜åŒ–             â”‚
â”‚  â”œâ”€â”€ èåˆç¼–ç å™¨                     å‡å°‘å†…å­˜åˆ†é…              â”‚
â”‚  â””â”€â”€ å†…å­˜ä¼˜åŒ–                       å¾ªç¯å¤„ç†é¿å…å¤§å¼ é‡        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ€§èƒ½æå‡æ€»ç»“

| ä¼˜åŒ–æŠ€æœ¯ | æ€§èƒ½æå‡ | é€‚ç”¨é˜¶æ®µ |
|---------|---------|---------|
| BF16 autocast | ~2x | å‰å‘+åå‘ |
| TensorCore (TF32) | ~8x | çŸ©é˜µä¹˜æ³• |
| Triton ç¨€ç–è§£ç  | 2-4x | è§£ç å™¨å‰å‘+åå‘ |
| èåˆç¼–ç å™¨ | 1.5-2x | ç¼–ç å™¨å‰å‘+åå‘ |
| ç¨€ç–æ¢¯åº¦ | k/M å€èŠ‚çœ | åå‘ä¼ æ’­ |
| TopK æ— æ’åº | 1.2-1.5x | TopK æ“ä½œ |

**æ€»ä½“åŠ é€Ÿ**: ç›¸æ¯”æœ´ç´  PyTorch å®ç°ï¼Œè®­ç»ƒé€Ÿåº¦æå‡ **4-8x**

---

## 2. è‡ªå®šä¹‰ Triton Kernel

### 2.1 æ ¸å¿ƒæ–‡ä»¶

**æ–‡ä»¶ä½ç½®**: `sparsify/xformers.py`

### 2.2 Triton Kernel æ¦‚è¿°

Sparsify ä½¿ç”¨è‡ªå®šä¹‰ Triton kernel å®ç°ç¨€ç–è§£ç æ“ä½œï¼Œè¿™æ˜¯ SAE è®­ç»ƒä¸­æœ€æ˜‚è´µçš„æ“ä½œä¹‹ä¸€ã€‚

#### å‰å‘ä¼ æ’­ Kernel

```python
# æ–‡ä»¶: xformers.py:10-31
@triton.jit
def embedding_bag_k(
    out_ptr,              # [B, dim] è¾“å‡º
    indices_ptr,          # [B, bag_size] top-k ç´¢å¼•
    weight_ptr,           # [num_latents, dim] è§£ç å™¨æƒé‡
    per_sample_weights,   # [B, bag_size] top-k æ¿€æ´»å€¼
    dim: tl.constexpr,
    dim_padded: tl.constexpr,  # å¯¹é½åˆ° 2 çš„å¹‚æ¬¡
    bag_size: tl.constexpr,    # k å€¼
):
    """
    åŠŸèƒ½ï¼šç¨€ç–è§£ç 
    out = Î£(weight[indices[i]] * per_sample_weights[i]) for i in [0, k)

    ä¼˜åŒ–ç‚¹ï¼š
    1. å†…å­˜å¯¹é½ï¼šdim_padded ä¿è¯åˆå¹¶è®¿é—®
    2. å¾ªç¯å±•å¼€ï¼šbag_size ä½œä¸ºç¼–è¯‘æ—¶å¸¸é‡
    3. FP32 ç´¯ç§¯ï¼šä¿è¯æ•°å€¼ç²¾åº¦
    """
    out_idx = tl.program_id(axis=0).to(tl.int64)
    out_value = tl.zeros([dim_padded], dtype=tl.float32)
    dim_mask = tl.arange(0, dim_padded) < dim

    # éå† k ä¸ªæ¿€æ´»çš„ latent
    for bag in range(0, bag_size):
        my_index = tl.load(indices_ptr + out_idx * bag_size + bag).to(tl.int64)
        my_scaling = tl.load(per_sample_weights + out_idx * bag_size + bag)
        my_weight = tl.load(
            weight_ptr + tl.arange(0, dim_padded) + my_index * dim,
            mask=dim_mask
        )
        out_value = out_value + my_weight.to(tl.float32) * my_scaling

    tl.store(
        out_ptr + out_idx * dim + tl.arange(0, dim_padded),
        out_value,
        mask=dim_mask
    )
```

**è°ƒç”¨å…¥å£**:
```python
# æ–‡ä»¶: xformers.py:34-53
def embedding_bag_triton(
    indices: Tensor,           # [B, k]
    weight: Tensor,            # [num_latents, dim]
    per_sample_weights: Tensor # [B, k]
) -> Tensor:
    trt_out = torch.empty(
        [indices.shape[0], weight.shape[1]],
        dtype=weight.dtype,
        device=weight.device
    )
    grid = (indices.shape[0],)  # æ¯ä¸ª batch ä¸€ä¸ª program

    embedding_bag_k[grid](
        trt_out,
        indices,
        weight,
        per_sample_weights,
        dim=weight.shape[-1],
        dim_padded=triton.next_power_of_2(weight.shape[-1]),  # å†…å­˜å¯¹é½
        bag_size=indices.shape[1],
        num_warps=1,
        num_stages=1,
    )
    return trt_out
```

#### åå‘ä¼ æ’­ Kernel

åå‘ä¼ æ’­æ›´å¤æ‚ï¼Œéœ€è¦å¤„ç†å¤šä¸ª batch æ›´æ–°åŒä¸€ä¸ª latent æƒé‡çš„æƒ…å†µã€‚

**æ­¥éª¤ 1: ç»Ÿè®¡æ¯ä¸ª embedding çš„ä½¿ç”¨æ¬¡æ•°**

```python
# æ–‡ä»¶: xformers.py:56-69
@triton.jit
def count_per_embedding_k(
    count_per_emb_ptr,  # [num_latents+1] (è¾“å‡º)
    indices_ptr,        # [B, k]
    bag_size: tl.constexpr,
):
    """ç»Ÿè®¡æ¯ä¸ª latent åœ¨å½“å‰ batch ä¸­è¢«æ¿€æ´»äº†å¤šå°‘æ¬¡"""
    batch_id = tl.program_id(axis=0).to(tl.int64)
    for i in range(bag_size):
        embedding_id = tl.load(indices_ptr + batch_id * bag_size + i)
        # åŸå­æ“ä½œï¼šå¤šä¸ªçº¿ç¨‹å¯èƒ½åŒæ—¶æ›´æ–°
        tl.atomic_add(
            count_per_emb_ptr + embedding_id + 1,
            1,
            sem="relaxed",
        )
```

**æ­¥éª¤ 2: æ„å»ºåå‘ç´¢å¼•æ˜ å°„**

```python
# æ–‡ä»¶: xformers.py:72-85
@triton.jit
def map_embeddings_and_outputs_k(
    reverse_mapping_ptr,     # [B*k] (è¾“å‡º)
    mapping_write_pos_ptr,   # [num_latents] (ä¸´æ—¶)
    indices_ptr,             # [B, k]
    bag_size: tl.constexpr,
):
    """
    ä¸ºæ¯ä¸ª latent è®°å½•å“ªäº› (batch, position) ä½¿ç”¨äº†å®ƒ
    è¿™æ ·åå‘ä¼ æ’­æ—¶å¯ä»¥å¿«é€ŸæŸ¥æ‰¾æ‰€æœ‰éœ€è¦ç´¯ç§¯æ¢¯åº¦çš„ä½ç½®
    """
    batch_id = tl.program_id(axis=0).to(tl.int64)
    for bag_id in range(bag_size):
        embedding_id = tl.load(indices_ptr + batch_id * bag_size + bag_id)
        write_pos = tl.atomic_add(
            mapping_write_pos_ptr + embedding_id, 1, sem="relaxed"
        )
        tl.store(reverse_mapping_ptr + write_pos, batch_id * bag_size + bag_id)
```

**æ­¥éª¤ 3: èšåˆæ¢¯åº¦**

```python
# æ–‡ä»¶: xformers.py:88-136
@triton.jit
def aggregate_gradient_for_embedding_k(
    weight_grad_ptr,                # [num_latents, dim] (è¾“å‡º)
    per_sample_weights_grad_ptr,    # [B, k] (è¾“å‡º)
    emb_argsorted_ptr,              # æ’åºåçš„ embedding ID
    weight_ptr,                     # [num_latents, dim]
    emb_begin_pos_ptr,              # æ¯ä¸ª embedding åœ¨åå‘æ˜ å°„ä¸­çš„èµ·å§‹ä½ç½®
    reverse_mapping_ptr,            # [B*k]
    per_sample_weights_ptr,         # [B, k]
    gradient_ptr,                   # [B, dim] æ¥è‡ªä¸Šæ¸¸çš„æ¢¯åº¦
    ...
):
    """
    å¯¹æ¯ä¸ª latent:
    1. æ‰¾åˆ°æ‰€æœ‰ä½¿ç”¨å®ƒçš„ (batch, position)
    2. ç´¯ç§¯æ¥è‡ªè¿™äº›ä½ç½®çš„æ¢¯åº¦åˆ° weight_grad
    3. è®¡ç®— per_sample_weights çš„æ¢¯åº¦

    ä¼˜åŒ–ï¼šæŒ‰ latent ä½¿ç”¨é¢‘ç‡æ’åºï¼Œå¹³è¡¡è´Ÿè½½
    """
    first_embedding_id = tl.program_id(axis=0).to(tl.int64)
    for k in range(0, BLOCK_SIZE):
        embedding_id = first_embedding_id + (K // BLOCK_SIZE) * k
        embedding_id = tl.load(emb_argsorted_ptr + embedding_id).to(tl.int64)

        weight_grad = tl.zeros([dim_padded], dtype=tl.float32)
        begin = tl.load(emb_begin_pos_ptr + embedding_id)
        end = tl.load(emb_begin_pos_ptr + embedding_id + 1)

        dim_mask = tl.arange(0, dim_padded) < dim
        weight = tl.load(
            weight_ptr + embedding_id * dim + tl.arange(0, dim_padded),
            mask=dim_mask,
        ).to(tl.float32)

        # éå†æ‰€æœ‰ä½¿ç”¨è¿™ä¸ª latent çš„ä½ç½®
        for idx in range(begin, end):
            output_indice_id = tl.load(reverse_mapping_ptr + idx).to(tl.int64)
            batch_id = output_indice_id // bag_size

            per_sample_w = tl.load(per_sample_weights_ptr + output_indice_id)
            gradient = tl.load(
                gradient_ptr + batch_id * dim + tl.arange(0, dim_padded),
                mask=dim_mask
            ).to(tl.float32)

            # ç´¯ç§¯æƒé‡æ¢¯åº¦
            weight_grad = weight_grad + per_sample_w * gradient

            # è®¡ç®— per_sample_weights æ¢¯åº¦
            per_sample_weights_grad = gradient * weight
            per_sample_weights_grad = tl.sum(per_sample_weights_grad)
            tl.store(
                per_sample_weights_grad_ptr + output_indice_id,
                per_sample_weights_grad
            )

        # å†™å…¥æƒé‡æ¢¯åº¦
        tl.store(
            weight_grad_ptr + embedding_id * dim + tl.arange(0, dim_padded),
            weight_grad,
            mask=dim_mask,
        )
```

**å®Œæ•´åå‘ä¼ æ’­å‡½æ•°**:

```python
# æ–‡ä»¶: xformers.py:139-185
def embedding_bag_bw_rev_indices(
    indices: Tensor,              # [B, k]
    weight: Tensor,               # [num_latents, dim]
    per_sample_weights: Tensor,   # [B, k]
    gradient: Tensor,             # [B, dim]
) -> tuple[Tensor, Tensor]:
    """
    è¿”å›: (weight.grad, per_sample_weights.grad)
    """
    K, dim = weight.shape
    B, bag_size = indices.shape

    # 1. ç»Ÿè®¡æ¯ä¸ª embedding çš„ä½¿ç”¨æ¬¡æ•°
    count_per_emb = torch.zeros((K + 1,), dtype=torch.uint32, device=indices.device)
    count_per_embedding_k[(B,)](count_per_emb, indices, bag_size=bag_size, num_warps=1)

    # 2. æŒ‰ä½¿ç”¨é¢‘ç‡æ’åºï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
    emb_argsorted = count_per_emb[1:].int().argsort(descending=True)
    emb_begin_pos = count_per_emb.cumsum(0)

    # 3. æ„å»ºåå‘æ˜ å°„
    reverse_mapping = torch.empty([B * bag_size], dtype=torch.uint32, device=indices.device)
    map_embeddings_and_outputs_k[(B,)](
        reverse_mapping_ptr=reverse_mapping,
        mapping_write_pos_ptr=emb_begin_pos.clone(),
        indices_ptr=indices,
        bag_size=bag_size,
        num_warps=1,
    )

    # 4. èšåˆæ¢¯åº¦
    weight_grad = torch.empty_like(weight)
    per_sample_weights_grad = torch.empty_like(per_sample_weights)
    BLOCK_SIZE = 8
    assert (K % BLOCK_SIZE) == 0
    aggregate_gradient_for_embedding_k[(K // BLOCK_SIZE,)](
        weight_grad_ptr=weight_grad,
        emb_begin_pos_ptr=emb_begin_pos,
        emb_argsorted_ptr=emb_argsorted,
        per_sample_weights_grad_ptr=per_sample_weights_grad,
        weight_ptr=weight,
        reverse_mapping_ptr=reverse_mapping,
        per_sample_weights_ptr=per_sample_weights,
        gradient_ptr=gradient,
        dim=dim,
        dim_padded=triton.next_power_of_2(dim),
        bag_size=bag_size,
        B=B,
        K=K,
        BLOCK_SIZE=BLOCK_SIZE,
        num_warps=1,
    )
    return weight_grad, per_sample_weights_grad
```

### 2.3 Triton vs PyTorch å¯¹æ¯”

```python
# PyTorch é»˜è®¤å®ç° (utils.py:174-177)
def eager_decode(top_indices, top_acts, W_dec):
    return nn.functional.embedding_bag(
        top_indices,
        W_dec.mT,
        per_sample_weights=top_acts,
        mode="sum"
    )

# Triton ä¼˜åŒ–å®ç° (utils.py:181-182)
def triton_decode(top_indices, top_acts, W_dec):
    return xformers_embedding_bag(top_indices, W_dec.mT, top_acts)
```

**æ€§èƒ½å·®å¼‚**:
- å‰å‘: Triton æ¯” PyTorch `embedding_bag` å¿« **2-3x**
- åå‘: Triton é€šè¿‡åå‘ç´¢å¼•ä¼˜åŒ–ï¼Œå¿« **3-4x**

### 2.4 ä¸ºä»€ä¹ˆ Triton æ›´å¿«ï¼Ÿ

1. **å†…å­˜å¯¹é½**: `dim_padded = triton.next_power_of_2(dim)` ç¡®ä¿åˆå¹¶å†…å­˜è®¿é—®
2. **ç¼–è¯‘æ—¶å¸¸é‡**: `bag_size` (å³ k) æ˜¯ç¼–è¯‘æ—¶å¸¸é‡ï¼Œå…è®¸å¾ªç¯å±•å¼€
3. **å‡å°‘åŸå­æ“ä½œå¼€é”€**: é€šè¿‡åå‘ç´¢å¼•é‡æ’ï¼Œå‡å°‘å†²çª
4. **ä¸“é—¨åŒ–**: é’ˆå¯¹ç¨€ç–è§£ç åœºæ™¯ä¼˜åŒ–ï¼Œè€Œ PyTorch å®ç°æ˜¯é€šç”¨çš„

### 2.5 åœ¨è®­ç»ƒä¸­çš„è°ƒç”¨ä½ç½®

```python
# sparse_coder.py:200-204
def decode(self, top_acts: Tensor, top_indices: Tensor) -> Tensor:
    assert self.W_dec is not None
    # decoder_impl ä¼šè‡ªåŠ¨é€‰æ‹© triton æˆ– eager å®ç°
    y = decoder_impl(top_indices, top_acts.to(self.dtype), self.W_dec.mT)
    return y + self.b_dec
```

```python
# utils.py:165-185
# è‡ªåŠ¨æ£€æµ‹æ˜¯å¦å¯ç”¨ Triton
try:
    from .xformers import xformers_embedding_bag
    decoder_impl = triton_decode
except ImportError:
    decoder_impl = eager_decode
```

---

## 3. èåˆç¼–ç å™¨ (Fused Encoder)

### 3.1 æ ¸å¿ƒæ–‡ä»¶

**æ–‡ä»¶ä½ç½®**: `sparsify/fused_encoder.py`

### 3.2 è®¾è®¡åŠ¨æœº

æ ‡å‡†å®ç°éœ€è¦ 3 ä¸ªç‹¬ç«‹æ“ä½œï¼š
```python
# æ ‡å‡†å®ç°ï¼ˆä¼ªä»£ç ï¼‰
x1 = F.linear(x, weight, bias)    # æ“ä½œ1: çº¿æ€§å˜æ¢
x2 = F.relu(x1)                    # æ“ä½œ2: ReLU æ¿€æ´»
top_acts, top_indices = torch.topk(x2, k)  # æ“ä½œ3: TopK é€‰æ‹©
```

**é—®é¢˜**:
- 3 æ¬¡å†…å­˜åˆ†é… (x1, x2, topk ç»“æœ)
- 3 æ¬¡ kernel å¯åŠ¨å¼€é”€
- åå‘ä¼ æ’­éœ€è¦ä¿å­˜ä¸­é—´ç»“æœ

**èåˆç¼–ç å™¨çš„è§£å†³æ–¹æ¡ˆ**:
- å°† 3 ä¸ªæ“ä½œèåˆæˆ 1 ä¸ª autograd Function
- å…±äº«åå‘ä¼ æ’­é€»è¾‘
- ç¨€ç–æ¢¯åº¦è®¡ç®—

### 3.3 å‰å‘ä¼ æ’­å®ç°

```python
# æ–‡ä»¶: fused_encoder.py:18-50
class FusedEncoder(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx,
        input,      # [N, D]
        weight,     # [M, D]
        bias,       # [M]
        k: int,
        activation: Literal["groupmax", "topk"]
    ):
        """
        èåˆ Linear â†’ ReLU â†’ TopK/GroupMax

        ä¼˜åŒ–ç‚¹ï¼š
        1. ä¸€æ¬¡å®Œæˆæ‰€æœ‰æ“ä½œ
        2. åªä¿å­˜å¿…è¦çš„ä¸­é—´ç»“æœï¼ˆindicesï¼‰
        3. é¿å…å¤šæ¬¡å†…å­˜åˆ†é…
        """
        # 1. Linear + ReLU èåˆ
        preacts = F.relu(F.linear(input, weight, bias))  # [N, M]

        # 2. TopK æˆ– GroupMax
        if activation == "topk":
            # sorted=False: ä¸éœ€è¦æ’åºï¼Œæ›´å¿«
            values, indices = torch.topk(preacts, k, dim=-1, sorted=False)
        elif activation == "groupmax":
            # GroupMax: å°† latents åˆ†æˆ k ç»„ï¼Œæ¯ç»„å–æœ€å¤§å€¼
            values, indices = preacts.unflatten(-1, (k, -1)).max(dim=-1)

            # ä¿®æ­£ç´¢å¼•ï¼šmax è¿”å›çš„æ˜¯ç»„å†…ç´¢å¼•ï¼Œéœ€è¦è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•
            num_latents = preacts.shape[1]
            offsets = torch.arange(
                0, num_latents, num_latents // k, device=preacts.device
            )
            indices = offsets + indices
        else:
            raise ValueError(f"Unknown activation: {activation}")

        # ä¿å­˜åå‘ä¼ æ’­æ‰€éœ€çš„å¼ é‡
        ctx.save_for_backward(input, weight, bias, indices)
        ctx.k = k
        return values, indices, preacts
```

**å…³é”®ä¼˜åŒ–**:
1. **`sorted=False`**: TopK ä¸æ’åºï¼Œåªæ‰¾å‡ºæœ€å¤§çš„ k ä¸ªå…ƒç´ ï¼Œé€Ÿåº¦æå‡ **1.2-1.5x**
2. **èåˆæ“ä½œ**: å‡å°‘ kernel å¯åŠ¨å’Œå†…å­˜åˆ†é…å¼€é”€
3. **æœ€å°åŒ–ä¿å­˜**: åªä¿å­˜ `indices`ï¼Œä¸ä¿å­˜å®Œæ•´çš„ `preacts`

### 3.4 åå‘ä¼ æ’­å®ç°

```python
# æ–‡ä»¶: fused_encoder.py:52-95
@staticmethod
def backward(ctx, grad_values, grad_indices, grad_preacts):
    """
    ä¼˜åŒ–çš„ç¨€ç–åå‘ä¼ æ’­

    å…³é”®ï¼šåªè®¡ç®— top-k ä¸ª latent çš„æ¢¯åº¦
    """
    input, weight, bias, indices = ctx.saved_tensors
    grad_input = grad_weight = grad_bias = None

    # ========== 1. å¯¹è¾“å…¥çš„æ¢¯åº¦ ==========
    if ctx.needs_input_grad[0]:
        # ä½¿ç”¨ embedding_bag è¿›è¡Œç¨€ç–æ”¶é›†
        # åªæœ‰è¢«é€‰ä¸­çš„ k ä¸ª latent ä¼šè´¡çŒ®æ¢¯åº¦
        grad_input = F.embedding_bag(
            indices,                              # [N, k]
            weight,                               # [M, D]
            mode="sum",
            per_sample_weights=grad_values.type_as(weight),  # [N, k]
        )
        # ç»“æœ: grad_input.shape = [N, D]

    # ========== 2. å¯¹æƒé‡çš„æ¢¯åº¦ ==========
    if ctx.needs_input_grad[1]:
        grad_weight = torch.zeros_like(weight)  # [M, D]
        k = ctx.k
        d_in = input.shape[-1]

        # ğŸ”¥ å†…å­˜ä¼˜åŒ–ï¼šå¾ªç¯å¤„ç† k ä¸ªä½ç½®
        # åŸå› ï¼šé¿å…åˆ†é… [..., k, d_in] å¤§å°çš„å¼ é‡ï¼ˆå¯èƒ½ 2-8 GBï¼‰
        # å½“å‰æ–¹æ³•ï¼šæ¯æ¬¡è¿­ä»£åªåˆ†é… [..., d_in] ï¼ˆçº¦ 64 MBï¼‰
        for i in range(k):
            # å–å‡ºç¬¬ i ä¸ªä½ç½®çš„æ¢¯åº¦å€¼å’Œç´¢å¼•
            grad_v = grad_values[..., i]        # [...] æ ‡é‡
            idx = indices[..., i]               # [...] ç´¢å¼•

            # å¤–ç§¯ï¼šgrad_v âŠ— input
            # grad_v: [...] â†’ [..., 1]
            # input: [..., d_in]
            # contrib: [..., d_in]
            contrib = grad_v.unsqueeze(-1) * input
            contrib = contrib.reshape(-1, d_in)

            # ç¨€ç–ç´¯ç§¯ï¼šåªæ›´æ–°è¢«æ¿€æ´»çš„ latent çš„æƒé‡
            grad_weight.index_add_(
                0,                              # æ²¿ latent ç»´åº¦
                idx.flatten(),                  # å±•å¹³çš„ç´¢å¼•
                contrib.type_as(weight)
            )
        # ç»“æœ: grad_weight.shape = [M, D]

    # ========== 3. å¯¹åç½®çš„æ¢¯åº¦ ==========
    if bias is not None and ctx.needs_input_grad[2]:
        grad_bias = torch.zeros_like(bias)  # [M]
        # åŒæ ·ä½¿ç”¨ç¨€ç–ç´¯ç§¯
        grad_bias.index_add_(
            0,
            indices.flatten(),
            grad_values.flatten().type_as(bias)
        )
        # ç»“æœ: grad_bias.shape = [M]

    # k å’Œ activation æ˜¯å¸¸é‡ï¼Œè¿”å› None
    return grad_input, grad_weight, grad_bias, None, None
```

**å†…å­˜ä¼˜åŒ–è¯¦è§£**:

```python
# ä¼ ç»Ÿæ–¹æ³•ï¼ˆå†…å­˜å¯†é›†ï¼‰:
# grad_weight = einsum('...k, ...d -> kd', grad_values, input)
# éœ€è¦å…ˆè®¡ç®— grad_values.unsqueeze(-1) * input.unsqueeze(-2)
# å½¢çŠ¶: [..., k, d_in]
# å†…å­˜: batch_size * seq_len * k * d_in * 4 bytes
# ä¾‹å¦‚: 32 * 2048 * 32 * 4096 * 4 = 34 GB âŒ

# èåˆç¼–ç å™¨æ–¹æ³•ï¼ˆå†…å­˜é«˜æ•ˆï¼‰:
for i in range(k):
    contrib = grad_values[..., i].unsqueeze(-1) * input
    # å½¢çŠ¶: [..., d_in]
    # å†…å­˜: batch_size * seq_len * d_in * 4 bytes
    # ä¾‹å¦‚: 32 * 2048 * 4096 * 4 = 1 GB âœ“
    grad_weight.index_add_(0, indices[..., i].flatten(), contrib)
```

èŠ‚çœæ˜¾å­˜: **2-8 GB**ï¼Œä½¿å¾—å¯ä»¥è®­ç»ƒæ›´å¤§çš„æ¨¡å‹æˆ–ä½¿ç”¨æ›´å¤§çš„ batch sizeã€‚

### 3.5 ä¾¿æ·åŒ…è£…å‡½æ•°

```python
# æ–‡ä»¶: fused_encoder.py:98-111
def fused_encoder(
    input,
    weight,
    bias,
    k: int,
    activation: Literal["groupmax", "topk"],
) -> EncoderOutput:
    """
    ä¾¿æ·åŒ…è£…ï¼Œè¿”å›å‘½åå…ƒç»„
    """
    return EncoderOutput(
        *FusedEncoder.apply(input, weight, bias, k, activation)
    )
```

### 3.6 åœ¨è®­ç»ƒä¸­çš„è°ƒç”¨ä½ç½®

```python
# sparse_coder.py:191-198
def encode(self, x: Tensor) -> EncoderOutput:
    """ç¼–ç è¾“å…¥å¹¶é€‰æ‹© top-k latents"""
    if not self.transcoder:
        x = x - self.b_dec  # autoencoder: ä¸­å¿ƒåŒ–

    # è°ƒç”¨èåˆç¼–ç å™¨
    return fused_encoder(
        x,
        self.encoder.weight,
        self.encoder.bias,
        self.cfg.k,
        self.cfg.activation
    )
```

---

## 4. æ··åˆç²¾åº¦è®­ç»ƒ (BF16)

### 4.1 æ ¸å¿ƒå®ç°

**æ–‡ä»¶ä½ç½®**: `sparse_coder.py:206-211`

```python
# Wrapping the forward in bf16 autocast improves performance by almost 2x
@torch.autocast(
    "cuda",
    dtype=torch.bfloat16,
    enabled=torch.cuda.is_bf16_supported(),
)
def forward(
    self, x: Tensor, y: Tensor | None = None, *, dead_mask: Tensor | None = None
) -> ForwardOutput:
    top_acts, top_indices, pre_acts = self.encode(x)
    # ... SAE å‰å‘ä¼ æ’­é€»è¾‘
```

### 4.2 BF16 vs FP32 vs FP16

| æ•°æ®ç±»å‹ | ç¬¦å·ä½ | æŒ‡æ•°ä½ | å°¾æ•°ä½ | åŠ¨æ€èŒƒå›´ | ç²¾åº¦ |
|---------|-------|-------|-------|---------|------|
| FP32    | 1     | 8     | 23    | Â±1.18e-38 ~ 3.40e38 | é«˜ |
| FP16    | 1     | 5     | 10    | Â±5.96e-8 ~ 6.55e4   | ä¸­ |
| BF16    | 1     | 8     | 7     | Â±1.18e-38 ~ 3.40e38 | ä¸­ |

**BF16 ä¼˜åŠ¿**:
1. **åŠ¨æ€èŒƒå›´**: ä¸ FP32 ç›¸åŒï¼Œä¸æ˜“ä¸Šæº¢/ä¸‹æº¢
2. **é€Ÿåº¦**: ä¸ FP16 ç›¸åŒï¼Œæ¯” FP32 å¿«çº¦ 2x
3. **å…¼å®¹æ€§**: æ¢¯åº¦ç´¯ç§¯å’ŒæŸå¤±è®¡ç®—æ›´ç¨³å®š

### 4.3 Autocast å·¥ä½œåŸç†

```python
@torch.autocast("cuda", dtype=torch.bfloat16)
def forward(self, x):
    # è‡ªåŠ¨è½¬æ¢è§„åˆ™:

    # 1. çŸ©é˜µä¹˜æ³• â†’ BF16
    y = F.linear(x, self.weight, self.bias)
    # x, weight, bias è‡ªåŠ¨è½¬ä¸º BF16
    # ç»“æœ y æ˜¯ BF16

    # 2. å½’çº¦æ“ä½œ â†’ FP32
    loss = y.pow(2).sum()
    # sum() åœ¨ FP32 ä¸­è¿›è¡Œï¼Œä¿è¯ç²¾åº¦

    # 3. TopK â†’ è¾“å…¥ç±»å‹
    top_values, top_indices = torch.topk(y, k)
    # ä½¿ç”¨ y çš„ç±»å‹ (BF16)

    return loss  # FP32
```

**è‡ªåŠ¨è½¬æ¢ç­–ç•¥**:
- **BF16 æ“ä½œ**: matmul, conv, linear
- **FP32 æ“ä½œ**: softmax, layer_norm, loss, sum/mean
- **ä¿æŒç±»å‹**: topk, argmax, indexing

### 4.4 æ€§èƒ½å½±å“

ä»£ç æ³¨é‡Šæ˜ç¡®æŒ‡å‡º: **"improves performance by almost 2x"**

åŸå› :
1. **å¸¦å®½å‡åŠ**: 2 å­—èŠ‚ vs 4 å­—èŠ‚
2. **TensorCore å‹å¥½**: Ampere+ GPU çš„ TensorCore åŸç”Ÿæ”¯æŒ BF16
3. **å¯„å­˜å™¨å‹åŠ›å‡å°**: æ›´å¤šæ•°æ®å¯ä»¥ç•™åœ¨å¯„å­˜å™¨ä¸­

### 4.5 æ•°å€¼ç¨³å®šæ€§

```python
# å…³é”®æ“ä½œä»ä½¿ç”¨ FP32
total_variance = (y - y.mean(0)).pow(2).sum()  # FP32
fvu = l2_loss / total_variance                 # FP32

# Triton kernel å†…éƒ¨ç´¯ç§¯ä¹Ÿç”¨ FP32
out_value = tl.zeros([dim_padded], dtype=tl.float32)  # FP32 ç´¯ç§¯
```

---

## 5. TensorCore åŠ é€Ÿ

### 5.1 å¯ç”¨ TF32 æ¨¡å¼

**æ–‡ä»¶ä½ç½®**: `trainer.py:355`

```python
# å¯ç”¨ TensorFloat-32 æ¨¡å¼
torch.set_float32_matmul_precision("high")
```

### 5.2 TF32 åŸç†

**TensorFloat-32 (TF32)** æ˜¯ NVIDIA Ampere æ¶æ„å¼•å…¥çš„æ–°æ•°æ®æ ¼å¼ï¼š

| æ ¼å¼  | ç¬¦å· | æŒ‡æ•° | å°¾æ•° | è¯´æ˜ |
|------|-----|-----|-----|------|
| FP32 | 1   | 8   | 23  | æ ‡å‡†å•ç²¾åº¦ |
| TF32 | 1   | 8   | 10  | TensorCore å†…éƒ¨æ ¼å¼ |
| BF16 | 1   | 8   | 7   | Brain Float 16 |

**TF32 ç‰¹ç‚¹**:
- è¾“å…¥/è¾“å‡ºä»æ˜¯ FP32
- TensorCore å†…éƒ¨ç”¨ TF32 è®¡ç®— (10 ä½å°¾æ•°)
- å¯¹ç”¨æˆ·é€æ˜ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 

### 5.3 æ€§èƒ½æå‡

åœ¨ Ampere (A100, RTX 3090) åŠä»¥ä¸Š GPU:
- **çŸ©é˜µä¹˜æ³•**: ~8x åŠ é€Ÿ (ç›¸æ¯” FP32 CUDA Core)
- **ååé‡**: 156 TFLOPS (A100, TF32) vs 19.5 TFLOPS (FP32)

```
æ ‡å‡† FP32 matmul (CUDA Core):
  19.5 TFLOPS (A100)

TF32 matmul (TensorCore):
  156 TFLOPS (A100)

åŠ é€Ÿæ¯”: 8x
```

### 5.4 ç²¾åº¦ vs æ€§èƒ½æƒè¡¡

```python
# ä¸‰ç§æ¨¡å¼
torch.set_float32_matmul_precision("highest")  # FP32, æœ€é«˜ç²¾åº¦ï¼Œæœ€æ…¢
torch.set_float32_matmul_precision("high")     # TF32, æ¨è âœ“
torch.set_float32_matmul_precision("medium")   # BF16, æœ€å¿«ï¼Œç²¾åº¦ç•¥ä½
```

Sparsify é€‰æ‹© `"high"` (TF32) æ˜¯å› ä¸ºï¼š
- ç²¾åº¦æŸå¤±å‡ ä¹ä¸å¯å¯Ÿè§‰ï¼ˆå°¾æ•°ä» 23 ä½é™åˆ° 10 ä½ï¼‰
- æ€§èƒ½æå‡æ˜¾è‘—ï¼ˆ8xï¼‰
- å…¼å®¹æ€§å¥½ï¼ˆä¸éœ€è¦ä¿®æ”¹ä»£ç ï¼‰

### 5.5 TF32 + BF16 ç»„åˆ

å½“åŒæ—¶ä½¿ç”¨ `@torch.autocast` å’Œ `set_float32_matmul_precision("high")`:

```python
@torch.autocast("cuda", dtype=torch.bfloat16)
def forward(self, x):
    # çŸ©é˜µä¹˜æ³•å®é™…ä½¿ç”¨çš„ç²¾åº¦:
    y = F.linear(x, self.weight)
    # x, weight è½¬ä¸º BF16
    # TensorCore å†…éƒ¨å¯èƒ½è¿›ä¸€æ­¥ä¼˜åŒ–ä¸º TF32/INT8
    # ç»“æœä¸º BF16
```

**å åŠ æ•ˆæœ**:
- Autocast: å¸¦å®½å‡åŠï¼ˆBF16ï¼‰ï¼Œ~2x åŠ é€Ÿ
- TF32: è®¡ç®—åŠ é€Ÿï¼ˆTensorCoreï¼‰ï¼Œ~8x åŠ é€Ÿ
- å®é™…: ç”±äºæ··åˆå› ç´ ï¼Œæ€»ä½“çº¦ **2-4x** åŠ é€Ÿ

---

## 6. ç¨€ç–æ¢¯åº¦è®¡ç®—

### 6.1 ç¨€ç–æ€§æ¥æº

SAE ä½¿ç”¨ TopK æ¿€æ´»ï¼Œåªæœ‰ k ä¸ª latent è¢«æ¿€æ´»ï¼š

```python
# å‰å‘ä¼ æ’­
pre_acts = ReLU(Linear(x))      # [batch, num_latents]  â† å…¨éƒ¨è®¡ç®—
top_acts, top_indices = topk(pre_acts, k)  # [batch, k]  â† åªé€‰ k ä¸ª

# å…³é”®ï¼šnum_latents = d_in * expansion_factor
# ä¾‹å¦‚ï¼šd_in=4096, expansion_factor=32 â†’ num_latents=131072
# k=32 â†’ ç¨€ç–åº¦ = 32/131072 = 0.024%
```

### 6.2 ç¨€ç–å‰å‘ä¼ æ’­

```python
# ç¼–ç å™¨: å…¨éƒ¨è®¡ç®— (æ— æ³•é¿å…)
pre_acts = F.linear(x, W_enc, b_enc)  # [N, M]
pre_acts = F.relu(pre_acts)           # [N, M]

# TopK: é€‰æ‹©ç¨€ç–æ¿€æ´»
top_acts, top_indices = torch.topk(pre_acts, k)  # [N, k]

# è§£ç å™¨: ç¨€ç–è®¡ç®— (åªè®¡ç®— k ä¸ª)
sae_out = 0
for i in range(k):
    idx = top_indices[:, i]       # [N]
    act = top_acts[:, i]          # [N]
    sae_out += W_dec[idx] * act   # åªè®¿é—® k ä¸ªæƒé‡è¡Œ
# ç­‰ä»·äº: embedding_bag(top_indices, W_dec, top_acts)
```

**è®¡ç®—é‡å¯¹æ¯”**:
```
ç¨ å¯†è§£ç  (å¦‚æœç”¨å®Œæ•´ matmul):
  [N, M] @ [M, D] = N * M * D æ¬¡ä¹˜æ³•

ç¨€ç–è§£ç  (embedding_bag):
  N * k * D æ¬¡ä¹˜æ³•

åŠ é€Ÿæ¯”: M / k = (D * expansion_factor) / k
       = (4096 * 32) / 32 = 4096x âœ“
```

### 6.3 ç¨€ç–åå‘ä¼ æ’­

#### 6.3.1 ç¼–ç å™¨æ¢¯åº¦

```python
# èåˆç¼–ç å™¨çš„ç¨€ç–åå‘ä¼ æ’­
# æ–‡ä»¶: fused_encoder.py:67-86

# å¯¹æƒé‡çš„æ¢¯åº¦: âˆ‚L/âˆ‚W_enc
# åªæœ‰è¢« TopK é€‰ä¸­çš„ latent æ‰æœ‰æ¢¯åº¦
for i in range(k):
    grad_v = grad_values[..., i]     # ç¬¬ i ä¸ªæ¿€æ´»çš„æ¢¯åº¦
    idx = indices[..., i]            # ç¬¬ i ä¸ªæ¿€æ´»çš„ç´¢å¼•

    contrib = grad_v.unsqueeze(-1) * input  # [N, D]
    grad_weight.index_add_(0, idx.flatten(), contrib)
    # åªæ›´æ–° indices[..., i] æŒ‡å‘çš„æƒé‡è¡Œ

# ç»“æœï¼š
# - grad_weight[j] = 0  å¦‚æœ latent j ä»æœªè¢«é€‰ä¸­
# - grad_weight[j] = Î£(grad * input)  å¦‚æœ latent j è¢«é€‰ä¸­
```

**ç¨€ç–åº¦**:
```
ç†è®ºä¸Š: åªæœ‰ k ä¸ª latent æœ‰æ¢¯åº¦
å®é™…ä¸Š: ç”±äºä¸åŒæ ·æœ¬é€‰ä¸­ä¸åŒ latentï¼Œ
        çº¦ batch_size * seq_len * k ä¸ªç‹¬ç‰¹ latent æœ‰æ¢¯åº¦

ä¾‹å¦‚: batch=32, seq=2048, k=32
     æœ€å¤š 32*2048*32 = 2,097,152 ä¸ªä½ç½®
     å¦‚æœ num_latents = 131072
     åˆ™å¹³å‡æ¯ä¸ª latent è¢«æ¿€æ´» ~16 æ¬¡

ä½†ä»ç„¶ç¨€ç–: ç›¸æ¯”ç¨ å¯†æ¢¯åº¦ (N * M)ï¼Œåªè®¡ç®—äº† (N * k)
```

#### 6.3.2 è§£ç å™¨æ¢¯åº¦

```python
# Triton å®ç°çš„ç¨€ç–åå‘ä¼ æ’­
# æ–‡ä»¶: xformers.py:88-136

# å¯¹ W_dec çš„æ¢¯åº¦: âˆ‚L/âˆ‚W_dec
# ä½¿ç”¨åå‘ç´¢å¼•æ˜ å°„
for each latent:
    if latent è¢«ä»»ä½•æ ·æœ¬æ¿€æ´»:
        # ç´¯ç§¯æ‰€æœ‰ä½¿ç”¨è¯¥ latent çš„ä½ç½®çš„æ¢¯åº¦
        for each position that used this latent:
            weight_grad[latent] += gradient[position] * activation[position]
    else:
        # æœªè¢«æ¿€æ´»çš„ latent æ²¡æœ‰æ¢¯åº¦
        weight_grad[latent] = 0
```

### 6.4 ç¨€ç–æ¢¯åº¦çš„ä¼˜åŠ¿

1. **è®¡ç®—èŠ‚çœ**:
   - å‰å‘è§£ç : `O(N * k * D)` è€Œé `O(N * M * D)`
   - åå‘ç¼–ç : åªæ›´æ–° `~(N * k)` ä¸ªæƒé‡è¡Œè€Œé `M` ä¸ª
   - åå‘è§£ç : ç±»ä¼¼

2. **å†…å­˜èŠ‚çœ**:
   - ä¸éœ€è¦å­˜å‚¨å®Œæ•´çš„ `[N, M]` æ¿€æ´»çŸ©é˜µ
   - åªå­˜å‚¨ `[N, k]` çš„ top æ¿€æ´»å’Œç´¢å¼•

3. **æ¢¯åº¦è´¨é‡**:
   - TopK è‡ªç„¶å®ç°ç‰¹å¾é€‰æ‹©
   - é¿å…äº† L1 æ­£åˆ™åŒ–çš„ bias

### 6.5 æ­»ç¥ç»å…ƒ (Dead Neurons) é—®é¢˜

ç¨€ç–è®­ç»ƒçš„å‰¯ä½œç”¨ï¼šæŸäº› latent å¯èƒ½æ°¸è¿œä¸è¢«æ¿€æ´»ã€‚

**è§£å†³æ–¹æ¡ˆ: AuxK æŸå¤±** (æ–‡ä»¶: `sparse_coder.py:233-253`)

```python
if dead_mask is not None and (num_dead := int(dead_mask.sum())) > 0:
    # å¯å‘å¼: ä½¿ç”¨è¾“å…¥ç»´åº¦ä¸€åŠçš„è¾…åŠ© k
    k_aux = y.shape[-1] // 2

    # åŠ¨æ€ç¼©æ”¾: å¦‚æœæ­»ç¥ç»å…ƒå°‘ï¼Œé™ä½æŸå¤±æƒé‡
    scale = min(num_dead / k_aux, 1.0)
    k_aux = min(k_aux, num_dead)

    # åªè€ƒè™‘æ­»ç¥ç»å…ƒï¼Œå…¶ä»–è®¾ä¸º -inf
    auxk_latents = torch.where(dead_mask[None], pre_acts, -torch.inf)

    # ä»æ­»ç¥ç»å…ƒä¸­é€‰ top-k_aux
    auxk_acts, auxk_indices = auxk_latents.topk(k_aux, sorted=False)

    # é¼“åŠ±è¿™äº›æ­»ç¥ç»å…ƒé¢„æµ‹ä¸» decoder çš„æ®‹å·®
    e_hat = self.decode(auxk_acts, auxk_indices)
    auxk_loss = (e_hat - e.detach()).pow(2).sum()
    auxk_loss = scale * auxk_loss / total_variance
```

è¿™æ ·å³ä½¿æŸäº› latent å½“å‰æœªè¢«ä¸» TopK é€‰ä¸­ï¼Œä»æœ‰æœºä¼šé€šè¿‡ AuxK æ¥æ”¶æ¢¯åº¦æ›´æ–°ã€‚

---

## 7. å…¶ä»–ä¼˜åŒ–æŠ€æœ¯

### 7.1 éƒ¨åˆ†å‰å‘ä¼ æ’­

**æ–‡ä»¶ä½ç½®**: `utils.py:113-153`, `trainer.py:759-762`

å¯¹äº FVU æŸå¤±ï¼ˆå±€éƒ¨é‡å»ºï¼‰ï¼Œä¸éœ€è¦å®Œæ•´è¿è¡Œæ¨¡å‹åˆ°æœ€åä¸€å±‚ã€‚

```python
def partial_forward_to_layer(
    model: PreTrainedModel,
    input_ids: Tensor,
    max_layer_idx: int
) -> dict[str, Tensor]:
    """åªè¿è¡Œåˆ° max_layer_idx å±‚å°±åœæ­¢"""

    # å®šä¹‰ hook æ‹¦æˆªå¹¶æå‰é€€å‡º
    def hook(module, input, output):
        if module_to_idx[module] == max_layer_idx:
            # æŠ›å‡ºå¼‚å¸¸ä»¥åœæ­¢å‰å‘ä¼ æ’­
            raise StopForwardException()

    # æ³¨å†Œ hooks
    handles = [mod.register_forward_hook(hook) for mod in modules]

    try:
        model(input_ids)
    except StopForwardException:
        pass  # æ­£å¸¸é€€å‡º
    finally:
        for h in handles:
            h.remove()
```

**ä½¿ç”¨åœºæ™¯**:
```python
# trainer.py:759-762
if self.cfg.loss_fn == "fvu":
    # å¦‚æœåªè®­ç»ƒå‰å‡ å±‚çš„ SAEï¼Œä¸éœ€è¦è¿è¡Œæ•´ä¸ªæ¨¡å‹
    max_layer = max(self.layer_to_idx[layer] for layer in self.cfg.hookpoints)
    outputs = partial_forward_to_layer(self.model, x, max_layer)
```

**æ€§èƒ½æå‡**:
- è®­ç»ƒ layer 0-5: èŠ‚çœ ~70% è®¡ç®—ï¼ˆå‡è®¾ 32 å±‚æ¨¡å‹ï¼‰
- è®­ç»ƒ layer 10-15: èŠ‚çœ ~50% è®¡ç®—

### 7.2 é«˜æ•ˆæ•°æ®åŠ è½½

#### å†…å­˜æ˜ å°„æ•°æ®é›†

**æ–‡ä»¶ä½ç½®**: `data.py:73-108`

```python
class MemmapDataset(TorchDataset):
    """é›¶æ‹·è´æ•°æ®åŠ è½½"""

    def __init__(self, data_path: str, ctx_len: int):
        # åªè¯»æ¨¡å¼ï¼Œä¸åŠ è½½åˆ°å†…å­˜
        self.mmap = np.memmap(data_path, dtype=np.uint16, mode="r")
        self.mmap = self.mmap.reshape(-1, ctx_len)

    def __getitem__(self, idx):
        # æŒ‰éœ€åŠ è½½ï¼Œæ“ä½œç³»ç»Ÿè‡ªåŠ¨ç®¡ç†ç¼“å­˜
        return {"input_ids": torch.from_numpy(self.mmap[idx].astype(np.int64))}
```

**ä¼˜åŠ¿**:
- ä¸å ç”¨å†…å­˜ï¼š100GB æ•°æ®é›†ä¸éœ€è¦ 100GB RAM
- åŠ è½½å³æ—¶ï¼šä¸éœ€è¦é¢„åŠ è½½
- ç³»ç»Ÿç®¡ç†ï¼šä¾èµ– OS é¡µé¢ç¼“å­˜

#### Token æ©ç 

**æ–‡ä»¶ä½ç½®**: `trainer.py:696-702`

```python
# æ’é™¤ç‰¹æ®Š token çš„æ¿€æ´»
exclude_special = self.cfg.exclude_special_tokens_from_activation
if exclude_special and (spc := self.special_tokens):
    # æ‰¾å‡ºç‰¹æ®Š token ä½ç½®
    is_special = torch.isin(x, spc)
    is_special = is_special.flatten(0, 1)

    # åœ¨ hook ä¸­ä½¿ç”¨ mask
    outputs = torch.where(is_special[..., None], torch.nan, outputs)
```

é¿å…åœ¨ `<PAD>`, `<EOS>` ç­‰ token ä¸Šè®­ç»ƒ SAEã€‚

### 7.3 ä¼˜åŒ–å™¨é€‰æ‹©

#### SignSGD ä¼˜åŒ–å™¨

**æ–‡ä»¶ä½ç½®**: `sign_sgd.py`

```python
class SignSGD(Optimizer):
    """Lâˆ èŒƒæ•°ä¸‹çš„æœ€é™¡ä¸‹é™"""

    def step(self):
        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is not None:
                    # åªä½¿ç”¨æ¢¯åº¦çš„ç¬¦å·ï¼Œå¿½ç•¥å¤§å°
                    p.add_(p.grad.sign(), alpha=-group["lr"])
```

**ä¼˜åŠ¿**:
- **å†…å­˜é«˜æ•ˆ**: æ—  momentum/stateï¼Œé›¶é¢å¤–å†…å­˜
- **ç¨³å®š**: å¯¹æ¢¯åº¦å°ºåº¦ä¸æ•æ„Ÿ
- **å¿«é€Ÿ**: è®¡ç®—ç®€å•ï¼Œåªéœ€ `sign()`

**é€‚ç”¨åœºæ™¯**: è¶…å®½ SAE (num_latents > 100k)

#### Muon ä¼˜åŒ–å™¨

**æ–‡ä»¶ä½ç½®**: `muon.py`

ä½¿ç”¨ Newton-Schulz æ­£äº¤åŒ–å®ç°è°±èŒƒæ•°çº¦æŸï¼š

```python
def quintic_newtonschulz(G: Tensor, steps: int) -> Tensor:
    """5 æ¬¡ Newton-Schulz è¿­ä»£æ­£äº¤åŒ–"""
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()  # åœ¨ BF16 ä¸­è®¡ç®—ï¼ˆæ›´å¿«ï¼‰
    X = X / X.norm(dim=(-2, -1), keepdim=True)

    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A
        X = a * X + B @ X

    return X
```

**ä¼˜åŠ¿**:
- æ›´å¥½çš„æ³›åŒ–æ€§èƒ½
- è‡ªé€‚åº”æ­¥é•¿
- å¯¹å­¦ä¹ ç‡ä¸æ•æ„Ÿ

**é€‚ç”¨åœºæ™¯**: å°åˆ°ä¸­ç­‰è§„æ¨¡ SAEï¼Œè¿½æ±‚æœ€ä½³æ€§èƒ½

### 7.4 æ¢¯åº¦å¤„ç†

#### è§£ç å™¨å½’ä¸€åŒ–çº¦æŸ

**æ–‡ä»¶ä½ç½®**: `sparse_coder.py:284-297`

```python
def remove_gradient_parallel_to_decoder_directions(self):
    """ç§»é™¤ä¸è§£ç å™¨æ–¹å‘å¹³è¡Œçš„æ¢¯åº¦åˆ†é‡"""
    # ä¿æŒ ||W_dec[i]|| = 1 çº¦æŸ

    # è®¡ç®—å¹³è¡Œåˆ†é‡
    parallel_component = einsum(
        self.W_dec.grad,
        self.W_dec.data,
        "d_sae d_in, d_sae d_in -> d_sae",
    )
    # å‡å»å¹³è¡Œåˆ†é‡ï¼Œåªä¿ç•™å‚ç›´åˆ†é‡
    self.W_dec.grad -= einsum(
        parallel_component,
        self.W_dec.data,
        "d_sae, d_sae d_in -> d_sae d_in",
    )
```

**ä½œç”¨**:
- ä¿æŒè§£ç å™¨æƒé‡å•ä½èŒƒæ•°
- æé«˜ç‰¹å¾çš„å¯è§£é‡Šæ€§
- é˜²æ­¢æƒé‡æ— é™å¢é•¿

#### å¾®æ‰¹æ¬¡ç´¯ç§¯

**æ–‡ä»¶ä½ç½®**: `trainer.py:781-804`

```python
for micro_step in range(self.cfg.micro_acc_steps):
    # å°† batch æ‹†åˆ†æˆæ›´å°çš„ micro-batch
    micro_batch = batch[start:end]

    # ä¸åŒæ­¥æ¢¯åº¦ï¼ˆDDPï¼‰
    with model.no_sync():
        loss = forward(micro_batch)
        loss.backward()

# ç´¯ç§¯å®Œæˆåå†åŒæ­¥
if ddp:
    for param in model.parameters():
        dist.all_reduce(param.grad)
```

**ä¼˜åŠ¿**:
- é™ä½å³°å€¼å†…å­˜
- å…è®¸æ›´å¤§çš„æœ‰æ•ˆ batch size
- åœ¨å†…å­˜å—é™çš„ GPU ä¸Šè®­ç»ƒå¤§æ¨¡å‹

---

## 8. å®Œæ•´å‰å‘åå‘æµç¨‹

### 8.1 æ•°æ®æµå›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        å®Œæ•´è®­ç»ƒæµç¨‹                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¾“å…¥: input_ids [B, S]  (B=batch_size, S=seq_len)
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Layer i (å†»ç»“)                                      â”‚
â”‚   hidden_states: [B, S, D]                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”‚ PyTorch Hook æ‹¦æˆª
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å±•å¹³: [B, S, D] â†’ [N, D]  where N = B * S                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã€ç¼–ç é˜¶æ®µã€‘ fused_encoder.py                                    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ ğŸ”¥ BF16 Autocast å¯ç”¨                                    â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                 â”‚
â”‚ x_centered = x - b_dec                     [N, D]              â”‚
â”‚   â”‚                                                             â”‚
â”‚   â”œâ”€â”€â–º Linear: pre_acts = x @ W_enc.T + b_enc                  â”‚
â”‚   â”‚             [N, D] @ [M, D].T â†’ [N, M]                     â”‚
â”‚   â”‚             (M = D * expansion_factor)                     â”‚
â”‚   â”‚                                                             â”‚
â”‚   â”œâ”€â”€â–º ReLU: pre_acts = max(0, pre_acts)   [N, M]              â”‚
â”‚   â”‚                                                             â”‚
â”‚   â””â”€â”€â–º TopK: top_acts, top_indices = topk(pre_acts, k)         â”‚
â”‚                [N, k]      [N, k]                              â”‚
â”‚                                                                 â”‚
â”‚   å…³é”®ä¼˜åŒ–:                                                      â”‚
â”‚   - sorted=False: ä¸æ’åºï¼Œåªæ‰¾æœ€å¤§ k ä¸ª                         â”‚
â”‚   - èåˆæ“ä½œ: ä¸€æ¬¡ kernel è°ƒç”¨                                  â”‚
â”‚   - BF16: å¸¦å®½å‡åŠ                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã€è§£ç é˜¶æ®µã€‘ xformers.py                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ ğŸš€ Triton Kernel: embedding_bag_k                        â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                 â”‚
â”‚ sae_out = Î£(W_dec[top_indices[i]] * top_acts[i]) + b_dec       â”‚
â”‚         = embedding_bag(top_indices, W_dec.T, top_acts)        â”‚
â”‚           [N, D]                                               â”‚
â”‚                                                                 â”‚
â”‚   å…³é”®ä¼˜åŒ–:                                                      â”‚
â”‚   - åªè®¿é—® k ä¸ª latent çš„æƒé‡ (ç¨€ç–)                            â”‚
â”‚   - å†…å­˜å¯¹é½: dim_padded = next_power_of_2(D)                  â”‚
â”‚   - FP32 ç´¯ç§¯: ä¿è¯ç²¾åº¦                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã€æŸå¤±è®¡ç®—ã€‘                                                     â”‚
â”‚                                                                 â”‚
â”‚ residual = x - sae_out                     [N, D]              â”‚
â”‚ total_variance = (x - x.mean(0)).pow(2).sum()                  â”‚
â”‚ l2_loss = residual.pow(2).sum()                                â”‚
â”‚ fvu = l2_loss / total_variance                                 â”‚
â”‚                                                                 â”‚
â”‚ # AuxK æŸå¤±ï¼ˆå¯é€‰ï¼Œæ¿€æ´»æ­»ç¥ç»å…ƒï¼‰                               â”‚
â”‚ if dead_mask is not None:                                      â”‚
â”‚     auxk_loss = compute_auxk_loss(...)                         â”‚
â”‚     total_loss = fvu + auxk_alpha * auxk_loss                  â”‚
â”‚ else:                                                           â”‚
â”‚     total_loss = fvu                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”‚ .backward()
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã€åå‘ä¼ æ’­ã€‘                                                     â”‚
â”‚                                                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ è§£ç å™¨åå‘ (xformers.py)                                 â”‚     â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚     â”‚
â”‚ â”‚ â”‚ ğŸš€ Triton: embedding_bag_bw_rev_indices         â”‚      â”‚     â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚     â”‚
â”‚ â”‚                                                         â”‚     â”‚
â”‚ â”‚ 1. ç»Ÿè®¡æ¯ä¸ª latent è¢«æ¿€æ´»æ¬¡æ•°                           â”‚     â”‚
â”‚ â”‚ 2. æ„å»ºåå‘ç´¢å¼•æ˜ å°„                                     â”‚     â”‚
â”‚ â”‚ 3. æŒ‰ latent èšåˆæ¢¯åº¦                                   â”‚     â”‚
â”‚ â”‚                                                         â”‚     â”‚
â”‚ â”‚ grad_W_dec[i] = Î£(gradient[j] * top_acts[j])           â”‚     â”‚
â”‚ â”‚                  for all j where top_indices[j] == i   â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ ç¼–ç å™¨åå‘ (fused_encoder.py)                            â”‚     â”‚
â”‚ â”‚                                                         â”‚     â”‚
â”‚ â”‚ grad_input:                                             â”‚     â”‚
â”‚ â”‚   = embedding_bag(top_indices, W_enc, grad_top_acts)    â”‚     â”‚
â”‚ â”‚   [N, D]  â† ç¨€ç–æ”¶é›†                                    â”‚     â”‚
â”‚ â”‚                                                         â”‚     â”‚
â”‚ â”‚ grad_W_enc: (å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬)                               â”‚     â”‚
â”‚ â”‚   for i in range(k):                                    â”‚     â”‚
â”‚ â”‚       contrib = grad_top_acts[...,i] âŠ— input            â”‚     â”‚
â”‚ â”‚       grad_W_enc.index_add_(0, top_indices[...,i],      â”‚     â”‚
â”‚ â”‚                             contrib)                    â”‚     â”‚
â”‚ â”‚   # å¾ªç¯ k æ¬¡ï¼Œæ¯æ¬¡åªåˆ†é… [N, D] è€Œé [N, k, D]          â”‚     â”‚
â”‚ â”‚   # èŠ‚çœ 2-8 GB æ˜¾å­˜                                     â”‚     â”‚
â”‚ â”‚                                                         â”‚     â”‚
â”‚ â”‚ grad_b_enc:                                             â”‚     â”‚
â”‚ â”‚   = scatter_add(top_indices, grad_top_acts)             â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã€æ¢¯åº¦å¤„ç† & ä¼˜åŒ–å™¨æ›´æ–°ã€‘                                        â”‚
â”‚                                                                 â”‚
â”‚ # 1. ç§»é™¤ä¸è§£ç å™¨æ–¹å‘å¹³è¡Œçš„æ¢¯åº¦                                  â”‚
â”‚ for sae in saes:                                               â”‚
â”‚     sae.remove_gradient_parallel_to_decoder_directions()       â”‚
â”‚                                                                 â”‚
â”‚ # 2. ä¼˜åŒ–å™¨æ­¥éª¤                                                 â”‚
â”‚ optimizer.step()  # SignSGD / Muon / Adam                      â”‚
â”‚                                                                 â”‚
â”‚ # 3. å½’ä¸€åŒ–è§£ç å™¨æƒé‡ï¼ˆå¯é€‰ï¼‰                                    â”‚
â”‚ if normalize_decoder:                                          â”‚
â”‚     sae.set_decoder_norm_to_unit_norm()                        â”‚
â”‚                                                                 â”‚
â”‚ # 4. æ¸…é›¶æ¢¯åº¦                                                   â”‚
â”‚ optimizer.zero_grad()                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 å…³é”®å¼ é‡å½¢çŠ¶è¿½è¸ª

```python
# å‡è®¾é…ç½®:
batch_size = 32
seq_len = 2048
d_in = 4096
expansion_factor = 32
k = 32

# è®¡ç®—ç»´åº¦
N = batch_size * seq_len = 65536
M = d_in * expansion_factor = 131072

# æµç¨‹ä¸­çš„å¼ é‡å½¢çŠ¶:
input_ids:        [32, 2048]           # è¾“å…¥ token IDs
hidden_states:    [32, 2048, 4096]     # Transformer è¾“å‡º
x:                [65536, 4096]        # å±•å¹³
pre_acts:         [65536, 131072]      # ç¼–ç å™¨é¢„æ¿€æ´»
top_acts:         [65536, 32]          # TopK æ¿€æ´»å€¼
top_indices:      [65536, 32]          # TopK ç´¢å¼•
sae_out:          [65536, 4096]        # SAE é‡å»º
fvu:              scalar               # æ ‡é‡æŸå¤±

# æƒé‡å½¢çŠ¶:
W_enc:            [131072, 4096]       # ç¼–ç å™¨æƒé‡
b_enc:            [131072]             # ç¼–ç å™¨åç½®
W_dec:            [131072, 4096]       # è§£ç å™¨æƒé‡
b_dec:            [4096]               # è§£ç å™¨åç½®

# æ¢¯åº¦å½¢çŠ¶ (ä¸æƒé‡ç›¸åŒ):
grad_W_enc:       [131072, 4096]
grad_b_enc:       [131072]
grad_W_dec:       [131072, 4096]
grad_b_dec:       [4096]
```

### 8.3 è®¡ç®—å¤æ‚åº¦åˆ†æ

```python
# ç¼–ç å™¨å‰å‘: O(N * M * D) = O(N * D^2 * expansion_factor)
pre_acts = x @ W_enc.T    # [N, D] @ [M, D].T = [N, M]
# æ“ä½œæ•°: N * M * D = 65536 * 131072 * 4096 â‰ˆ 35T FLOPs

# TopK: O(N * M * log(k))
top_acts, top_indices = topk(pre_acts, k)
# æ“ä½œæ•°: N * M * log(k) â‰ˆ 65536 * 131072 * 5 â‰ˆ 43G FLOPs

# è§£ç å™¨å‰å‘: O(N * k * D) (ç¨€ç–ï¼)
sae_out = embedding_bag(top_indices, W_dec.T, top_acts)
# æ“ä½œæ•°: N * k * D = 65536 * 32 * 4096 â‰ˆ 8.6G FLOPs
#         vs ç¨ å¯† matmul: N * M * D â‰ˆ 35T FLOPs
#         åŠ é€Ÿæ¯”: 4096x âœ“

# æ€»å‰å‘: ~35T FLOPs (ç¼–ç å™¨ä¸»å¯¼)
# æ€»åå‘: ~70T FLOPs (çº¦ä¸ºå‰å‘çš„ 2x)
# æ€»è®¡: ~105T FLOPs per step
```

**å®é™…è®­ç»ƒé€Ÿåº¦**:
```
åœ¨ A100 (80GB) ä¸Š:
- BF16 TensorCore: 312 TFLOPS (ç†è®ºå³°å€¼)
- å®é™…åˆ©ç”¨ç‡: ~40-50%
- æœ‰æ•ˆåå: ~120-150 TFLOPS
- æ—¶é—´/step: 105T / 150T â‰ˆ 0.7 ç§’

ä¼˜åŒ–å:
- æ‰¹æ¬¡: 32 * 2048 = 65536 tokens
- ååé‡: 65536 / 0.7 â‰ˆ 93k tokens/sec
```

---

## 9. æ€§èƒ½åŸºå‡†ä¸è°ƒä¼˜

### 9.1 æ€§èƒ½åŸºå‡†

åœ¨ A100 (80GB) GPU ä¸Šçš„å…¸å‹æ€§èƒ½ï¼ˆå•å¡ï¼‰:

| æ¨¡å‹ | d_in | expansion | k | batch | ååé‡ | æ˜¾å­˜ |
|------|------|-----------|---|-------|--------|------|
| GPT2-small | 768 | 32 | 32 | 32 | 140k tok/s | 12 GB |
| Pythia-160M | 768 | 32 | 32 | 32 | 135k tok/s | 13 GB |
| Pythia-1B | 2048 | 32 | 32 | 16 | 65k tok/s | 28 GB |
| Llama-7B | 4096 | 32 | 32 | 8 | 22k tok/s | 55 GB |
| Llama-13B | 5120 | 32 | 32 | 4 | 11k tok/s | 72 GB |

### 9.2 æ˜¾å­˜ä½¿ç”¨åˆ†æ

```python
# ä¸»è¦æ˜¾å­˜å ç”¨:
æ¨¡å‹æƒé‡ (å†»ç»“):     ~æ¨¡å‹å¤§å° (ä¾‹å¦‚ Llama-7B: ~14GB in BF16)
SAE æƒé‡:            4 * d_in * M * num_layers
                     = 4 * 4096 * 131072 * 1 / 1e9
                     â‰ˆ 2.1 GB per layer

æ¿€æ´» (å‰å‘):         N * d_in * 4 bytes
                     = 65536 * 4096 * 4 / 1e9
                     â‰ˆ 1 GB

ä¸­é—´æ¿€æ´» (SAE):      N * M * 4 bytes (pre_acts)
                     = 65536 * 131072 * 4 / 1e9
                     â‰ˆ 34 GB  â† æœ€å¤§ï¼

ä¼˜åŒ–ç­–ç•¥:
1. BF16: å‡åŠ â†’ 17 GB
2. ä¸ä¿å­˜å®Œæ•´ pre_acts: åªä¿å­˜ top_indices â†’ 0.5 GB
3. micro_acc_steps: åˆ†æ‰¹å¤„ç† â†’ è¿›ä¸€æ­¥å‡å°‘
```

### 9.3 è°ƒä¼˜å»ºè®®

#### 9.3.1 æœ€å¤§åŒ–ååé‡

```bash
# 1. ä½¿ç”¨æœ€å¤§å¯èƒ½çš„ batch size
python -m sparsify model_name --batch_size 64

# 2. å¯ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆå¦‚æœæ˜¾å­˜ä¸è¶³ï¼‰
python -m sparsify model_name --batch_size 32 --grad_acc_steps 2

# 3. ä½¿ç”¨å¾®æ‰¹æ¬¡ç´¯ç§¯ï¼ˆè¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜ï¼‰
python -m sparsify model_name --batch_size 32 --micro_acc_steps 4

# 4. åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¤š GPUï¼‰
torchrun --nproc_per_node 8 -m sparsify model_name --batch_size 8
```

#### 9.3.2 æœ€å°åŒ–æ˜¾å­˜

```bash
# 1. å‡å° batch size
--batch_size 8

# 2. ä½¿ç”¨ 8bit é‡åŒ–åŠ è½½æ¨¡å‹
--load_in_8bit

# 3. å‡å° expansion factorï¼ˆç‰ºç‰²å®¹é‡ï¼‰
--expansion_factor 16

# 4. åˆ†å¸ƒå¼æ¨¡å—ï¼ˆæ¯ä¸ª GPU åªè®­ç»ƒéƒ¨åˆ†å±‚ï¼‰
torchrun --nproc_per_node 4 -m sparsify model_name \
    --distribute_modules --layers 0 4 8 12
```

#### 9.3.3 ä¼˜åŒ–å™¨é€‰æ‹©

```python
# SignSGD: æœ€çœå†…å­˜ï¼Œè®­ç»ƒç¨³å®š
--optimizer signum --lr 5e-3

# Adam: å¹³è¡¡æ€§èƒ½å’Œæ”¶æ•›é€Ÿåº¦
--optimizer adam --lr 2e-4

# Muon: æœ€ä½³æ€§èƒ½ï¼Œä½†éœ€è¦æ›´å¤šæ˜¾å­˜
--optimizer muon --lr 2e-3
```

### 9.4 æ€§èƒ½åˆ†æå·¥å…·

```python
# ä½¿ç”¨ PyTorch Profiler åˆ†ææ€§èƒ½
from torch.profiler import profile, ProfilerActivity

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
    profile_memory=True,
    with_stack=True
) as prof:
    # è®­ç»ƒä¸€ä¸ª step
    trainer.fit_one_step()

# è¾“å‡ºåˆ†æ
print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=20))
prof.export_chrome_trace("trace.json")  # åœ¨ chrome://tracing ä¸­æŸ¥çœ‹
```

å…¸å‹çš„æ—¶é—´åˆ†å¸ƒ:
```
æ“ä½œ                    æ—¶é—´å æ¯”
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ç¼–ç å™¨å‰å‘ (matmul)      35%
è§£ç å™¨å‰å‘ (triton)      15%
TopK                     8%
ç¼–ç å™¨åå‘              25%
è§£ç å™¨åå‘              12%
å…¶ä»– (æ•°æ®åŠ è½½ç­‰)        5%
```

### 9.5 å·²çŸ¥æ€§èƒ½ç“¶é¢ˆä¸è§£å†³æ–¹æ¡ˆ

| ç“¶é¢ˆ | ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|-----|------|---------|
| æ˜¾å­˜ä¸è¶³ | OOM é”™è¯¯ | å‡å° batch_size, ä½¿ç”¨ micro_acc_steps |
| GPU åˆ©ç”¨ç‡ä½ | <50% | å¢å¤§ batch_size, æ£€æŸ¥æ•°æ®åŠ è½½ |
| TopK æ…¢ | TopK å ç”¨ >15% æ—¶é—´ | å‡å° expansion_factor æˆ–ä½¿ç”¨ groupmax |
| ç¼–ç å™¨æ…¢ | matmul å ç”¨ >50% æ—¶é—´ | ç¡®è®¤ TF32 å·²å¯ç”¨ï¼Œæ£€æŸ¥ BF16 autocast |
| è§£ç å™¨æ…¢ | embedding_bag >20% | ç¡®è®¤ Triton kernel å·²å®‰è£…å¹¶ä½¿ç”¨ |

---

## æ€»ç»“

Sparsify çš„é«˜é€Ÿè®­ç»ƒæ¥è‡ªå¤šå±‚æ¬¡ä¼˜åŒ–çš„ååŒä½œç”¨ï¼š

### æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯

1. **Triton è‡ªå®šä¹‰ Kernel** (`xformers.py`)
   - ç¨€ç–è§£ç å‰å‘/åå‘
   - åå‘ç´¢å¼•æ˜ å°„
   - å†…å­˜å¯¹é½ä¼˜åŒ–

2. **èåˆç¼–ç å™¨** (`fused_encoder.py`)
   - Linear + ReLU + TopK èåˆ
   - ç¨€ç–æ¢¯åº¦è®¡ç®—
   - å†…å­˜ä¼˜åŒ–ï¼ˆå¾ªç¯å¤„ç†ï¼‰

3. **æ··åˆç²¾åº¦è®­ç»ƒ** (BF16)
   - ~2x æ•´ä½“åŠ é€Ÿ
   - å¸¦å®½å‡åŠ
   - æ•°å€¼ç¨³å®š

4. **TensorCore åŠ é€Ÿ** (TF32)
   - ~8x matmul åŠ é€Ÿ
   - é€æ˜ä½¿ç”¨
   - ç²¾åº¦æŸå¤±å°

5. **ç®—æ³•çº§ç¨€ç–æ€§**
   - TopK æ¿€æ´»
   - ç¨€ç–å‰å‘/åå‘
   - 4096x è§£ç åŠ é€Ÿ

### æ€§èƒ½æå‡æ€»ç»“

ç›¸æ¯”æœ´ç´  PyTorch å®ç°:
- **è®­ç»ƒé€Ÿåº¦**: 4-8x æå‡
- **æ˜¾å­˜æ•ˆç‡**: 2-8 GB èŠ‚çœ
- **ååé‡**: åœ¨ A100 ä¸Šè¾¾åˆ° 90k+ tokens/sec (Llama-7B)

### å…³é”®æ–‡ä»¶é€ŸæŸ¥

| æ–‡ä»¶ | å…³é”®å‡½æ•° | ä½œç”¨ |
|-----|---------|------|
| `xformers.py` | `embedding_bag_triton` | Triton ç¨€ç–è§£ç å‰å‘ |
| | `embedding_bag_bw_rev_indices` | Triton ç¨€ç–è§£ç åå‘ |
| `fused_encoder.py` | `FusedEncoder.forward` | èåˆç¼–ç å‰å‘ |
| | `FusedEncoder.backward` | ç¨€ç–ç¼–ç åå‘ |
| `sparse_coder.py` | `@torch.autocast` | BF16 æ··åˆç²¾åº¦ |
| `trainer.py` | `set_float32_matmul_precision` | TF32 å¯ç”¨ |
| `utils.py` | `decoder_impl` | è‡ªåŠ¨é€‰æ‹©è§£ç å®ç° |

---

## å‚è€ƒèµ„æº

- **Triton æ–‡æ¡£**: https://triton-lang.org/
- **PyTorch Autograd**: https://pytorch.org/docs/stable/notes/extending.html
- **BF16 Training**: https://pytorch.org/docs/stable/amp.html
- **TensorCore**: https://www.nvidia.com/en-us/data-center/tensor-cores/

---

*æ–‡æ¡£æ›´æ–°æ—¥æœŸ: 2026-01-08*
